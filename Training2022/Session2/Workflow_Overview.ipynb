{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical examples of simple workflow building and integration with PyOphidia and ESDM-PAV Client modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will show the basic usage and some advanced features (dependency specification, iterative and parallel interfaces, selection interface, task error handling) related to the Ophidia workflows.\n",
    "\n",
    "We will be using **PyOphidia** module and the **ESDM-PAV Client**, a Python module providing the features to model and execute a *Post-processing, Analytics and Visualisation* (PAV) experiment (https://github.com/OphidiaBigData/esdm-pav-client).\n",
    "\n",
    "It implements three main classes:\n",
    "\n",
    "- **Workflow**: it submits, cancels and monitors an PAV experiment execution (a workflow)\n",
    "- **Experiment**: it creates or loads a PAV experiment that is a sequence of tasks\n",
    "- **Task**: it creates a Task object that can be embedded in an PAV experiment workflow\n",
    "\n",
    "First of all, import **ESDM-PAV Client** module and **PyOphidia** module setting up a connection to the **Ophidia Server**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esdm_pav_client import Workflow, Experiment, Task\n",
    "import sys\n",
    "from PyOphidia import cube,client\n",
    "cube.Cube.setclient(read_env=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you restart the container in the right folder. The cube space should be completely **empty**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.list(level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:7px;border-top:2px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic workflow: import + subset + export\n",
    "\n",
    "Create a simple PAV experiment consisting of 3 tasks: \n",
    "- **one independent task**: *Import* to import a NetCDF file into a datacube\n",
    "- **two dependent tasks**: *Subset* and *Export* to perform a subsetting operation along the dimensions of the  datacube and export the result into a new NetCDF file.\n",
    "\n",
    "\n",
    "<img src=\"imgs/1_Basic_workflow.svg\" alt=\"basic_workflow\" width=\"100\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start building the first workflow step by step!\n",
    "\n",
    "First of all, we define in the experiment a few **global attributes**, which include a number of metadata and default parameters values common to all the tasks. \n",
    "Some of these keywords are mandatory:\n",
    "- ```name```: the title of the workflow\n",
    "- ```author```: the author’s name\n",
    "- ```abstract```: a short description of the workflow\n",
    "\n",
    "The parameter ```exec_mode``` specifies the execution mode, synchronous or asynchronous, and it refers to the entire workflow, not to single tasks. In case of synchronous mode the workflow will be executed in a blocking way, so the submitter will have to wait until it will be finished to display the results. If the execution mode is asynchronous, the workflow will be processed in a non-blocking way (like a batch mode), allowing the submitter to immediately take the control and eventually submit other requests.\n",
    "\n",
    "Since a lot of tasks could be launched in parallel, an important parameter is the number of cores per task (```ncores```) which specifies the default value to be associated with all the workflow’s tasks. This value can be overridden with another one tailored to a task with a different behaviour.\n",
    "\n",
    "By using the ```on_exit``` parameter the user can select the cubes that will be dropped out when a workflow ends.\n",
    "The default value of ```on_exit``` can be set as global attribute. Admitted values are:\n",
    "- oph_delete: remove the output cube\n",
    "- oph_deletecontainer: remove the output container (valid only for OPH_CREATECONTAINER)\n",
    "- nop: no operation has to be applied (default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = Experiment(\n",
    "    name= \"Basic workflow\",\n",
    "    author= \"CMCC\",\n",
    "    abstract= \"Perform some basics operations using workflows\",\n",
    "    exec_mode= \"sync\",\n",
    "    ncores=\"2\",\n",
    "    on_exit=\"oph_delete\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each task is uniquely identified within the workflow by its ```name``` and it is related to a specific Ophidia Operator set as ```operator```. According to that operator, the user can optionally insert an array of key-value pairs (```arguments```) in order to call the operator with the appropriate arguments.\n",
    "\n",
    "The ```type``` field can be used to specify the tool for task execution (e.g. Ophidia or CDO), or to identify tasks that “control” the flow execution, as for example: iterative loops, parallel branches, selection statements, waits for data availability, etc. \n",
    "\n",
    "The first task of the workflow is related to the *oph_importnc* operator. According to the operator specification, we need to specify the mandatory arguments: *input* (i.e. the src_path) and *measure*. In this example, the two arguments values, ```$1``` and ```$2```, will be replaced with the workflow input parameters before sending the request to the Ophidia Server.\n",
    "\n",
    "In addition, we can specify other arguments, such as:\n",
    "- *import_metadata=yes* to import also metadata from the input NetCDF file\n",
    "- *imp_dim=time* to arrange data in order to operate on time series\n",
    "- *imp_concept_level=d* to set the concept level to *day*\n",
    "- *description* to add a datacube description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = e1.newTask(name=\"Import\",\n",
    "               type=\"ophidia\",\n",
    "               operator='oph_importnc',\n",
    "               arguments={'measure': '$2',\n",
    "                          'import_metadata': 'yes',\n",
    "                          'imp_dim': 'time', \n",
    "                          'imp_concept_level': 'd',\n",
    "                          'hierarchy': 'oph_base|oph_base|oph_time',\n",
    "                          'description': 'Max Temp', \n",
    "                          'input': '/home/ophidia/notebooks/$1'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second task is related to the *oph_subset* operator. \n",
    "\n",
    "For example, we can consider the whole spatial domain and specify a subset only on the time range, as indicated by the *subset_dims* parameter.\n",
    "We can select a particular season by using the corresponding code for the *subset filter* argument:\n",
    " - *DJF* for winter\n",
    " - *MAM* for spring\n",
    " - *JJA* for summer\n",
    " - *SON* for autumn\n",
    "\n",
    "The ```dependencies``` argument can be used to specify a list of dependencies in a compact format using a Python dictionary structure. Each dependency can be defined by specifying the task object to be linked (e.g., task *t1*) followed by the input argument in the dependent task.\n",
    "The input cube is the output cube of the previous task (*Import*), so we have to specify a dependency between these two tasks, so the ```cube``` parameter of the *oph_subset* operator will be set to the PID of the cube imported in the *Import* task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = e1.newTask(name=\"Subset\",\n",
    "               type=\"ophidia\", \n",
    "               operator='oph_subset', \n",
    "               arguments={'subset_filter': 'JJA', 'subset_dims': 'time', 'subset_type': 'coord', \n",
    "                          'description': 'JJA'},\n",
    "               dependencies={t1:'cube'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can export the subsetted cube by using the *oph_exportnc2* operator.\n",
    "\n",
    "In this case we have to:\n",
    "- provide the argument for the oph_exportnc2 operator: *output*\n",
    "- set a dependency from the previous (i.e. Subset) task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = e1.newTask(name=\"Export\",\n",
    "               type=\"ophidia\",\n",
    "               operator='oph_exportnc2', \n",
    "               arguments={'output': '/home/ophidia/notebooks/JJA.nc'},\n",
    "               dependencies={t2:'cube'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first workflow is ready!\n",
    "\n",
    "We can save it as a JSON object by using the ```save``` method, in case we need to submit it later from a batch script. The JSON file is created in the same folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1.save(\"simple_exp\")\n",
    "print(open(\"simple_exp.json\", \"r\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the ```check``` method, the user can verify the experiment structure before submitting the execution.\n",
    "The argument *visual* (set to ```True```) can be used to visualise the experiment definition graph as a picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1.check(visual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before submitting the experiment, we have to define its input arguments:\n",
    "- *NetCDF filename* under the */home/ophidia/notebooks/* folder\n",
    "- *variable* to be imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=\"tasmax_day_CMCC-CESM_rcp85_r1i1p1_20960101-21001231.nc\"\n",
    "variable=\"tasmax\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a new workflow has been instantiated from an experiment definition, it can be executed with the ```submit``` method, providing the input parameters (```$1``` and ```$2```):\n",
    "- the source path used in the oph_importnc operator to load the NetCDF file\n",
    "- the variable name used to set the measure argument in the import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = Workflow(e1)\n",
    "w1.submit(file,variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the ```monitor``` method allows users to check the experiment execution progress and the status of all the tasks composing the experiment; the status can be displayed in a graphical format, similar to what shown by the check method.\n",
    "The function takes 3 optional parameters:\n",
    "- *frequency* parameter, i.e., an integer that determines how often the status is updated\n",
    "- *iterative* parameter which defines if the status has to be periodically updated (with ```True```) or not (with ```False```)\n",
    "- *visual_mode* parameter which defines if the status will be displayed via a graph (with ```True```), otherwise the status will be provided by text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1.monitor(frequency=1, iterative=True, visual_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check exported file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -lh /home/ophidia/notebooks | grep \"\\.nc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cube workspace is already empty (we have no datacubes) because we've used ```\"on_exit\": \"oph_delete\"``` as workflow global attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.list(level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just need to remove the container automatically created by the *oph_importnc* operator. By default, the container name is equal to the name of the imported file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.deletecontainer(container=file,force='yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:7px;border-top:2px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Workflows: iterative and parallel interfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now consider a slightly more complex workflow, in which we are going to:\n",
    "\n",
    "- import and subset multiple NetCDF files\n",
    "- merge all the subsetted datacubes\n",
    "- perform a reduction (avg, max, min, ...) operation\n",
    "- export the output datacube\n",
    "\n",
    "<img src=\"imgs/2_Iterative_parallel.svg\" alt=\"iterative_parallel\" width=\"900\">\n",
    "\n",
    "As input files, we can use the daily NetCDF files produced by the CMCC-CESM model and related to the *tasmax* variable for the years 2096-2100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /home/ophidia/notebooks | grep \"tasmax\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the Ophidia Data Import/Export & analysis operators, we are going to exploit the ```for``` and ```endfor``` flow control operators to implement a **for** loop. \n",
    "\n",
    "Unlike other operators, these do not operate on data or metadata, but can be adopted to set particular flow control rules for the Workflow manager. In particular, the operators are used to begin/end a sub-section that has to be executed several times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the workflow step by step.\n",
    "\n",
    "In the following cell we define some global attributes as in the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2 = Experiment(\n",
    "    name=\"Loop operations\",\n",
    "    author=\"CMCC\",\n",
    "    abstract=\"Perform some basics operations using workflows\",\n",
    "    exec_mode=\"sync\",\n",
    "    ncores=\"1\",\n",
    "    on_exit=\"oph_delete\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a new container**\n",
    "\n",
    "This is the first task, so it has no dependencies. We just have to provide the proper arguments to the *oph_createcontainer* operator:\n",
    "- the container name\n",
    "- the name and the type of the dimensions allowed\n",
    "- the concept hierarchy name of the dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = e2.newTask(name=\"Create container\",\n",
    "                type=\"ophidia\",\n",
    "                operator='oph_createcontainer',\n",
    "                on_error='skip',\n",
    "                arguments={'container': 'workflow',\n",
    "                           'dim': 'lat|lon|time',\n",
    "                           'dim_type': 'double|double|double',\n",
    "                           'hierarchy': 'oph_base|oph_base|oph_time'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FOR statement**\n",
    "\n",
    "The FOR operator is used to configure the iterative block and, in particular, to set the number N of loops to be executed. By this aim, we have to provide an ordered list of N labels to be assigned to cycles in order to distinguish one cycle from another one. The list is assigned to the ```values``` parameter, separating each value by | (\"pipe\"). \n",
    "\n",
    "In our example, we provide a list of years in order to import the corresponding NetCDF file in the next task.\n",
    "\n",
    "A name has to be associated to the list values by setting the ```key``` parameter (e.g. ```year```), which is used in the inner tasks in the form **@{key_name}** to access the current value of the counter/label. \n",
    "\n",
    "The ```type=\"control\"``` identifies a flow control operator.  \n",
    "\n",
    "Finally, we define a simple flow dependency (```t1:''```), since this task has to begin only after the previous \"Create container\" task has finished, but no data is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = e2.newTask(name=\"Start loop\",\n",
    "                type=\"control\",\n",
    "                operator='for',\n",
    "                arguments={\"key\": \"year\", \"values\": \"2096|2097|2098|2099|2100\"},\n",
    "                dependencies={t1:''})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import and subset multiple datacubes in parallel**\n",
    "\n",
    "The two inner tasks to be repeated (import and subset) have to depend on FOR task directly or indirectly, namely they depend on other tasks in the iterative block. \n",
    "\n",
    "Setting the parameters of these tasks the user is able to exploit the value of the label associated with current iteration. \n",
    "\n",
    "*IMPORT task*\n",
    "\n",
    "The **input** as well the **description** parameters in the *oph_importnc* operator are defined in a parametrized way to get the current value of the **year** key for each iteration.\n",
    "\n",
    "This task has a flow dependency from the \"Start loop\" task in order to start after this task and retrieve the right value of the label associated with current iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = e2.newTask(name=\"Import\",\n",
    "                type=\"ophidia\",\n",
    "                operator='oph_importnc',\n",
    "                arguments={'measure': 'tasmax',\n",
    "                           'container': 'workflow',\n",
    "                           'import_metadata': 'yes',\n",
    "                           'imp_dim': 'time', \n",
    "                           'imp_concept_level': 'd',\n",
    "                           'hierarchy': 'oph_base|oph_base|oph_time',\n",
    "                           'description': 'Max Temp @{year}', \n",
    "                           'input': '/home/ophidia/notebooks/tasmax_day_CMCC-CESM_rcp85_r1i1p1_@{year}0101-@{year}1231.nc'},\n",
    "                dependencies={t2:''})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*SUBSET task*\n",
    "\n",
    "This task has a dependency from the \"Import\" task since each subset operation has to be performed on the corresponding datacube imported at the previous import step, so the output from the *Import* task is the input for the *Subset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4 = e2.newTask(name=\"Subset\",\n",
    "                type=\"ophidia\",\n",
    "                operator='oph_subset', \n",
    "                arguments={'subset_filter': 'JJA', \n",
    "                           'subset_dims': 'time', \n",
    "                           'subset_type': 'coord', \n",
    "                           'description': 'JJA @{year}'},\n",
    "                dependencies={t3:'cube'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End loop**\n",
    "\n",
    "The ENDFOR operator ends an iterative block, has no arguments and depends on the inner tasks.\n",
    "\n",
    "In our example, it depends on the \"Subset\" task. In this way, it can gather PIDs of all cubes generated by the (subset) inner task and transfer them to next tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5 = e2.newTask(name=\"End loop\",\n",
    "                type=\"control\",\n",
    "                operator='endfor',\n",
    "                arguments={},\n",
    "                dependencies={t4:'cube'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge all the subsetted datacubes into a single datacube**\n",
    "\n",
    "All the subsetted datacubes can be now merged into a single datacube by using the **oph_mergecubes** operator: the resulting datacube will contain the JJA subset for each of the imported years. \n",
    "\n",
    "As for the previous task, we need to specify a dependency to get all the datacubes PIDs from the previous task. In addition, we have to set the ```argument``` parameter to ```cubes``` so that the value of the *cubes* parameter for the *oph_mergecubes* operator will be set to a list of pipe-separated PIDs retrieved from the \"End loop\" task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t6 = e2.newTask(name=\"Merge\",\n",
    "                type=\"ophidia\",\n",
    "                operator='oph_mergecubes', \n",
    "                arguments={'description': 'Merged cube'}, \n",
    "                dependencies={t5:'cubes'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform a reduction operation**\n",
    "\n",
    "Starting from the merged datacube, we can perform a reduction operation with respect to the implicit dimension (time).\n",
    "\n",
    "We just need to define a datacube dependency between the **Reduce** task and the previous **Merge** task.\n",
    "\n",
    "The reduced cube will contain the average value for the tasmax variable over the 2096-2100 JJA period for each point in the spatial domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t7 = e2.newTask(name=\"Reduce\",\n",
    "                type=\"ophidia\",\n",
    "                operator='oph_reduce', \n",
    "                arguments={'operation': 'avg', \n",
    "                           'dim': 'time',\n",
    "                           'description': 'Reduced cube'},\n",
    "                dependencies={t6:'cube'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export the averaged datacube**\n",
    "\n",
    "In a similar way, we can define an *\\\"Export\\\"* task that depends on the *\\\"Reduce\\\"* task to export data into a single NetCDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t8 = e2.newTask(name=\"Export\",\n",
    "                type=\"ophidia\",\n",
    "                operator='oph_exportnc2', \n",
    "                arguments={'output': '/home/ophidia/notebooks/avg_JJA.nc'},\n",
    "                dependencies={t7:'cube'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Empty workspace**\n",
    "\n",
    "Finally, we can remove the container created by the first task of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t9 = e2.newTask(name=\"Delete container\",\n",
    "                type=\"ophidia\",\n",
    "                operator='oph_deletecontainer', \n",
    "                arguments={'container': 'workflow', \n",
    "                           'force': 'yes'},\n",
    "                dependencies={t8:''})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run workflow**\n",
    "\n",
    "We can submit the workflow and monitor the experiment execution progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = Workflow(e2)\n",
    "w2.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2.monitor(frequency=1, iterative=True, visual_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check exported file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -lh /home/ophidia/notebooks/ | grep \"\\.nc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:7px;border-top:2px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel loop statement\n",
    "\n",
    "The FOR operator used to configure the iterative block can also be executed in parallel to speed up the execution, if there is no dependency between the iteration branches.\n",
    "\n",
    "In this case we set ```parallel``` argument to ```yes``` for parallel processing.\n",
    "\n",
    "If this option is enabled, the engine, before executing the workflow, transforms it into an equivalent version in which iterative blocks are expanded into N independent sub-workflows, where N is the number of initial iterations. The new workflow is then executed taking into account the usual rules based on task dependencies.\n",
    "\n",
    "For example in the previous definition for the \"Start loop\" task:\n",
    "\n",
    "```\n",
    "t2 = e3.newTask(name=\"Start loop\",\n",
    "                type=\"control\",\n",
    "                operator='for',\n",
    "                arguments={\"key\": \"year\", \"values\": \"2096|2097|2098|2099|2100\", \n",
    "                           'parallel': 'yes'},\n",
    "                dependencies={t1:''})\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The resulting workflow with parallel loop is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e3 = Experiment(\n",
    "    name=\"Loop operations\",\n",
    "    author=\"CMCC\",\n",
    "    abstract=\"Perform some basics operations using workflows\",\n",
    "    exec_mode=\"sync\",\n",
    "    ncores=\"1\",\n",
    "    on_exit=\"oph_delete\"\n",
    ")\n",
    "t1 = e3.newTask(name=\"Create container\",\n",
    "                type=\"ophidia\",\n",
    "                operator='oph_createcontainer',\n",
    "                on_error='skip',\n",
    "                arguments={'container': 'workflow',\n",
    "                           'dim': 'lat|lon|time',\n",
    "                           'dim_type': 'double|double|double',\n",
    "                           'hierarchy': 'oph_base|oph_base|oph_time'})\n",
    "t2 = e3.newTask(name=\"Start loop\",\n",
    "                type=\"control\",\n",
    "                operator='for',\n",
    "                arguments={\"key\": \"year\", \"values\": \"2096|2097|2098|2099|2100\",\n",
    "                           'parallel': 'yes'},\n",
    "                dependencies={t1:''})\n",
    "t3 = e3.newTask(name=\"Import\",\n",
    "                type=\"ophidia\",\n",
    "                operator='oph_importnc',\n",
    "                arguments={'measure': 'tasmax',\n",
    "                           'container': 'workflow',\n",
    "                           'import_metadata': 'yes',\n",
    "                           'imp_dim': 'time', \n",
    "                           'imp_concept_level': 'd',\n",
    "                           'hierarchy': 'oph_base|oph_base|oph_time',\n",
    "                           'description': 'Max Temp @{year}', \n",
    "                           'input': '/home/ophidia/notebooks/tasmax_day_CMCC-CESM_rcp85_r1i1p1_@{year}0101-@{year}1231.nc'},\n",
    "                dependencies={t2:''})\n",
    "t4 = e3.newTask(name=\"Subset\",\n",
    "                type=\"ophidia\",\n",
    "                operator='oph_subset', \n",
    "                arguments={'subset_filter': 'JJA', \n",
    "                           'subset_dims': 'time', \n",
    "                           'subset_type': 'coord', \n",
    "                           'description': 'JJA @{year}'},\n",
    "                dependencies={t3:'cube'})\n",
    "t5 = e3.newTask(name=\"End loop\",\n",
    "                type=\"control\",\n",
    "                operator='endfor',\n",
    "                arguments={},\n",
    "                dependencies={t4:'cube'})\n",
    "t6 = e3.newTask(name=\"Merge\",\n",
    "                type=\"ophidia\",\n",
    "                operator='oph_mergecubes', \n",
    "                arguments={'description': 'Merged cube'}, \n",
    "                dependencies={t5:'cubes'})\n",
    "t7 = e3.newTask(name=\"Reduce\",\n",
    "                type=\"ophidia\",\n",
    "                operator='oph_reduce', \n",
    "                arguments={'operation': 'avg', \n",
    "                           'dim': 'time',\n",
    "                           'description': 'Reduced cube'},\n",
    "                dependencies={t6:'cube'})\n",
    "t8 = e3.newTask(name=\"Export\",\n",
    "                type=\"ophidia\",\n",
    "                operator='oph_exportnc2', \n",
    "                arguments={'output': '/home/ophidia/notebooks/avg_JJA_parallel.nc'},\n",
    "                dependencies={t7:'cube'})\n",
    "t9 = e3.newTask(name=\"Delete container\",\n",
    "                type=\"ophidia\",\n",
    "                operator='oph_deletecontainer', \n",
    "                arguments={'container': 'workflow', \n",
    "                           'force': 'yes'},\n",
    "                dependencies={t8:''})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run workflow**\n",
    "\n",
    "We can submit the workflow using the **submit** method again\n",
    "\n",
    "*Note the execution time.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w3 = Workflow(e3)\n",
    "w3.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the experiment execution graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w3.monitor(frequency=1, iterative=True, visual_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check exported file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -lh /home/ophidia/notebooks/ | grep \"\\.nc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:7px;border-top:2px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. Workflows: Selection interface\n",
    "\n",
    "The Selection interface provides further flexibility by enabling the Workflow manager to execute one or more tasks based on boolean conditions that could be checked at run-time and depend on input parameters, data, metadata, etc.\n",
    "\n",
    "The development of the Selection interface involves specific control operators:\n",
    " - IF\n",
    " - ELSEIF\n",
    " - ELSE\n",
    " - ENDIF\n",
    "\n",
    "Similarly to other flow control operators, they does not process data or metadata directly, but they could be adopted to enable (or to skip) the execution of a set of tasks based on run-time conditions.\n",
    "\n",
    "In the following workflow, we'll consider a selection statement with two selection blocks.\n",
    "\n",
    "<img src=\"imgs/3_Selection_Interface.svg\" alt=\"selection_interface\" width=\"800\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Workflow global attributes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e4 = Experiment(\n",
    "    name=\"Selection Interface\",\n",
    "    author=\"CMCC\",\n",
    "    abstract=\"Selection statement with two selection blocks\",\n",
    "    exec_mode=\"sync\",\n",
    "    ncores=\"1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IF block**\n",
    "\n",
    "The selection interface is used to code two possible implementations of a task that imports data into the Ophidia platform from an external source:\n",
    " 1. import only the subset from the input file\n",
    " 2. import all the dataset and then extract a data subset\n",
    "\n",
    "The actual implementation to be adopted is selected by means of the input parameter ```$1```: a numerical non-zero value for option A, 0 for option B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = e4.newTask(name=\"IF block\",\n",
    "                type=\"control\",\n",
    "                operator='if',\n",
    "                arguments={'condition': '$1'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CASE A: Import only the subset from the input file**\n",
    "\n",
    "In general, the set of tasks belonging to the branch that begins from IF and ends to ENDIF is the sub-workflow to be executed in case the condition set for IF is satisfied.\n",
    "\n",
    "In our example, there is only one task, named *\\\"Import and subset\\\"*, which is related to the *oph_importnc* operator and has a flow dependency from the \"IF block\" task.\n",
    "\n",
    "The *input* and the *measure* arguments will be set according to the second and third workflow input arguments (```$2``` and ```$3```).\n",
    "\n",
    "To import only a subset from the input file we have to specify in addition the following parameters:\n",
    "- **subset_dims**: the dimension names used for the subsetting;\n",
    "- **subset_type=coord** so that the filter is considered on dimension values;\n",
    "- **subset_filter**: list of pipe-separated filters associated to each dimension specified in *subset_dims* (set according to the fourth workflow input argument ```$4```).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = e4.newTask(name=\"Import and subset\",\n",
    "                type=\"ophidia\",\n",
    "                operator='oph_importnc',\n",
    "                arguments={'measure': '$3', \n",
    "                           'import_metadata': 'yes',\n",
    "                           'imp_dim': 'time', \n",
    "                           'imp_concept_level': 'd',\n",
    "                           'hierarchy': 'oph_base|oph_base|oph_time',\n",
    "                           'description': 'Max Temp imported and subsetted',\n",
    "                           'subset_dims': 'lat|lon|time',\n",
    "                           'subset_filter': '$4',\n",
    "                           'subset_type': 'coord',\n",
    "                           'time_filter': 'no',\n",
    "                           'input': '/home/ophidia/notebooks/$2'},\n",
    "                dependencies={t1:''})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ELSE block**\n",
    "\n",
    "The task with the ELSE operator has to be a child of the task with the IF operator. It has no arguments: it simply starts the last sub-block of a selection block \"if\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = e4.newTask(name=\"ELSE block\",\n",
    "                type=\"control\",\n",
    "                operator='else',\n",
    "                arguments={},\n",
    "                dependencies={t1:''})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CASE B: import all the dataset and then extract a data subset**\n",
    "\n",
    "The set of tasks belonging to the branch that begins from ELSE and ends to ENDIF is the sub-workflow to be executed in case the condition set for IF is not satisfied.\n",
    "\n",
    "In our example, we have two tasks:\n",
    "- the first one, **\\\"Import data\\\"**, is related to the *oph_importnc* operator and is child of the task with the \"ELSE\" operator.\n",
    "- the second one,**\\\"Subset data\\\"**, is related to the *oph_subset* operator and has a dependency from the \"Import data\" task since the input datacube to be subsetted is the datacube generated from the import task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4 = e4.newTask(name=\"Import data\",\n",
    "                type=\"ophidia\",\n",
    "                operator='oph_importnc',\n",
    "                arguments={'measure': '$3', \n",
    "                           'import_metadata': 'yes',\n",
    "                           'imp_dim': 'time', \n",
    "                           'imp_concept_level': 'd',\n",
    "                           'hierarchy': 'oph_base|oph_base|oph_time',\n",
    "                           'description': 'Max Temp imported',\n",
    "                           'input': '/home/ophidia/notebooks/$2'},\n",
    "                dependencies={t3:''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5 = e4.newTask(name=\"Subset data\",\n",
    "                type=\"ophidia\",\n",
    "                operator='oph_subset', \n",
    "                arguments={'subset_dims': 'lat|lon|time', \n",
    "                           'subset_filter': '$4', \n",
    "                           'subset_type': 'coord', \n",
    "                           'description': 'Max Temp subsetted'},\n",
    "                dependencies={t4:'cube'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ENDIF block**\n",
    "\n",
    "The *endif* operator simply closes a selection block \"if\".\n",
    "\n",
    "If we want to gather the PID of the output datacube produced in each of the two branches, we have to specify a dependency from both final tasks (*\\\"Subset data\\\"* and *\\\"Import and subset\\\"*) of each sub-workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t6 = e4.newTask(name=\"Selection block end\",\n",
    "                type=\"control\",\n",
    "                operator='endif',\n",
    "                arguments={},\n",
    "                dependencies={t2:'', t5:''})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the workflow arguments:\n",
    "- *if condition* ```$1```\n",
    "- *NetCDF filename* ```$2```\n",
    "- *variable* to be imported ```$3```\n",
    "- *subset filter* (lat|lon|time) ```$4```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition=0\n",
    "file=\"tasmax_day_CMCC-CESM_rcp85_r1i1p1_20960101-21001231.nc\"\n",
    "variable=\"tasmax\"\n",
    "subset=\"-50:10|20:140|150:240\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w4 = Workflow(e4)\n",
    "w4.submit(condition, file, variable, subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and monitor it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w4.monitor(frequency=1, iterative=True, visual_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check produced datacube. As we can note:\n",
    "- if **condition** equal **1** ---> datacube is imported and subsetted at the same time\n",
    "- else ---> datacube is first imported, then subsetted\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.list(level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the subsetted datacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsetted_cube = cube.Cube(pid='http://127.0.0.1/ophidia/.../...')\n",
    "subsetted_cube.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and delete the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.deletecontainer(container=file,force='yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:7px;border-top:2px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling task errors\n",
    "\n",
    "The Workflow manager provides also support for handling errors in task executions.\n",
    "\n",
    "Four behaviours are supported by setting the proper value in the ```on_error``` argument:\n",
    "- *skip*: the task is skipped and execution continues on the descendant tasks\n",
    "- *continue*: the task and all depending task will be ignored, while other tasks will be executed\n",
    "- *abort*: the workflow is interrupted (default)\n",
    "- *repeat N*: the task is re-executed N times\n",
    "\n",
    "In all the previous examples the default behaviour of interrupting the workflow in case of errors (*abort*) was used.\n",
    "\n",
    "Note that the behaviour can be defined both at general workflow level and at the task level.\n",
    "\n",
    "We can now define a workflow where some of the tasks are expected to fail.  In particular, we want to import NetCDF files related to different years and compute the average on the time dimensions. Let's suppose we try to access a file that does not exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the global workflow attributes and the ```on_error``` argument through the ```$1``` variable to control the general workflow execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e5 = Experiment(\n",
    "    name=\"error_handling\",\n",
    "    author=\"CMCC\",\n",
    "    abstract=\"Perform some basics operations using workflows\",\n",
    "    exec_mode=\"sync\",\n",
    "    ncores=\"1\",\n",
    "    on_error=\"$1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define then the first two tasks: \"Create container\" and \"Start loop\". \n",
    "\n",
    "**Note** that the ```values``` argument points to year (*2095*) for which we do not have any file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = e5.newTask(name=\"Create container\",\n",
    "                type=\"ophidia\",\n",
    "                operator='oph_createcontainer',\n",
    "                arguments={'container': 'workflow',\n",
    "                           'dim': 'lat|lon|time',\n",
    "                           'dim_type': 'double|double|double',\n",
    "                           'hierarchy': 'oph_base|oph_base|oph_time'})\n",
    "t2 = e5.newTask(name=\"Start loop\",\n",
    "                type=\"control\",\n",
    "                operator='for',\n",
    "                arguments={\"key\": \"year\", \"values\": \"2095|2096|2097\", \n",
    "                           'parallel': 'yes'},\n",
    "                dependencies={t1:''})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now specify the import operator. Here we are defining a specific ```on_error``` behaviour at the task level, with ```$2```, which supersedes the one define at the global level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = e5.newTask(name=\"Import\",\n",
    "                type=\"ophidia\",\n",
    "                operator='oph_importnc',\n",
    "                on_error='$2',\n",
    "                arguments={'measure': 'tasmax', \n",
    "                           'container': 'workflow',\n",
    "                           'import_metadata': 'yes',\n",
    "                           'imp_dim': 'time', \n",
    "                           'imp_concept_level': 'd',\n",
    "                           'hierarchy': 'oph_base|oph_base|oph_time',\n",
    "                           'description': 'Max Temp @{year}',\n",
    "                           'input': '/home/ophidia/notebooks/tasmax_day_CMCC-CESM_rcp85_r1i1p1_@{year}0101-@{year}1231.nc',},\n",
    "                dependencies={t2:''})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then specify the remaining tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4 = e5.newTask(name=\"Reduce\",\n",
    "                type=\"ophidia\",\n",
    "                operator='oph_reduce', \n",
    "                arguments={'operation': 'avg',\n",
    "                           'dim': 'time',\n",
    "                           'description': 'Reduced cube'},\n",
    "                dependencies={t3:'cube'})\n",
    "t5 = e5.newTask(name=\"End loop year\",\n",
    "                type=\"control\",\n",
    "                operator='endfor',\n",
    "                arguments={},\n",
    "                dependencies={t4:'cube'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to run it with the default behaviour and see what happens..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w5 = Workflow(e5)\n",
    "w5.submit('abort','abort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w5.monitor(frequency=1, iterative=True, visual_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could then set a specific behaviour for the whole workflow to skip a failed task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w5 = Workflow(e5)\n",
    "w5.submit('skip','skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w5.monitor(frequency=1, iterative=True, visual_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or define a specific one for the import operator we expect to fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w5 = Workflow(e5)\n",
    "w5.submit('skip','continue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w5.monitor(frequency=1, iterative=True, visual_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the cubes created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.list(level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the last example, empty the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.deletecontainer(container=\"workflow\",force='yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:7px;border-top:2px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Heat Waves Duration Indices as a workflow of operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now make a workflow of Ophidia operators to include the computation of some Heat Waves Duration Indices:\n",
    "- the **HWD (Heat Wave Duration)**: starting from the daily maximum temperature TSMX, the Heat Wave Duration index is the maximum number of days at intervals of at least 6 days with $TSMX > 5°C$ + average calculated for each calendar day (based on 20 years 1980 - 2000) using a current 5-day window\n",
    "- the **HWF (Heat Wave Frequency)**: the number of days that contribute to heatwaves in a year\n",
    "- the **HWN (Heat Wave Number)**: the number of heatwaves in a year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task list includes the following tasks:\n",
    "\n",
    "1. **Create container**\n",
    " - the oph_createcontainer operator creates a container called \"Heatwaves\"\n",
    "\n",
    "2. **Import climatological mean**\n",
    " - the climatological mean represents the average computed for each calendar day (based on 20 years)\n",
    " - the input NetCDF data set located at ```input``` (set to the last workflow input parameter) is imported into the Ophidia platform, with maximum temperature in °K (see http://ophidia.cmcc.it/documentation/users/operators/OPH_IMPORTNC.html)\n",
    " - the ```measure``` is set to *tasmax*\n",
    " - data is arranged in order to operate on time series (as indicated by the ```imp_dim``` parameter)\n",
    " - the task has a dependency from the previous task. \n",
    " \n",
    "3. **Import**\n",
    " - the input NetCDF data set located at ```input``` is imported into the Ophidia platform, with maximum temperature in °K and represent the new year on which compute the indicators\n",
    " - the task has a dependency from the **Create container** task and it is executed in parallel with the second task\n",
    " \n",
    "4. **Intercube**\n",
    " - the *oph_intercube* operator (see http://ophidia.cmcc.it/documentation/users/operators/OPH_INTERCUBE.html) is used to subtract the elements of the second cube from the first cube one by one\n",
    " - the task has a dependency from the **Import climatological mean** task and another from the **Import** task\n",
    " \n",
    "\n",
    "5. **Apply**\n",
    " - the *oph_apply* operator (see http://ophidia.cmcc.it/documentation/users/operators/OPH_APPLY.html) is used to compute a sequence of operations in order to identifies the Heat Wave durations: $\\{day \\mid TSMX(day) > 5°C\\}$ \n",
    " - a dependency from **Intercube** task\n",
    " \n",
    "6. **Reduce**\n",
    " - the *oph_reduce* operator (see http://ophidia.cmcc.it/documentation/users/operators/OPH_REDUCE.html) is used with ```operation=max``` and ```dim=time``` to extract the maximum duration (**HWDI**)\n",
    " - a dependency from the previous task\n",
    " \n",
    "7. **Apply for HWN**\n",
    " - the *oph_apply* operator is used for basically creating a mask by using the *oph_predicate* primitive (see http://ophidia.cmcc.it/documentation/users/primitives/OPH_PREDICATE.html) setting to 1 the durations greater than 0\n",
    " - a dependency from **Reduce** task\n",
    " \n",
    "8. **Reduce for HWN**\n",
    " - the *oph_reduce2* operator is used with ```operation=sum``` and ```dim=time``` to count the days of heatwaves in a year (**HWN**)\n",
    " - a dependency from the previous task\n",
    " \n",
    "9. **Reduce for HWF**\n",
    " - the *oph_reduce2* operator is used with ```operation=sum``` and ```dim=time``` to sum the heatwaves durations\n",
    " - a dependency from the **Reduce** task\n",
    " \n",
    "10. **Apply for HWF**\n",
    " - the *oph_apply* operator is used to divide the sum of durations by 365 using the *oph_mul_scalar* primitive (see http://ophidia.cmcc.it/documentation/users/primitives/OPH_MUL_SCALAR.html) to identify the **HWF** index\n",
    " - a dependency from **Reduce for HWF** task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e6 = Experiment(\n",
    "    name=\"Heat Waves\",\n",
    "    author=\"CMCC\",\n",
    "    abstract=\"Perform the computation of Heat Waves indices using workflows\",\n",
    "    exec_mode=\"sync\",\n",
    "    ncores=\"1\",\n",
    ")\n",
    "t1 = e6.newTask(name=\"Create container\",\n",
    "                type=\"ophidia\",\n",
    "                operator='oph_createcontainer',\n",
    "                on_error='skip',\n",
    "                arguments={'container': 'heatwaves',\n",
    "                           'dim': 'lat|lon|time',\n",
    "                           'dim_type': 'double|double|double',\n",
    "                           'hierarchy': 'oph_base|oph_base|oph_time'})\n",
    "t2 = e6.newTask(name=\"Import climatological mean\",\n",
    "                type=\"ophidia\",\n",
    "                operator='oph_importnc',\n",
    "                arguments={'measure': 'tasmax',\n",
    "                           'container': 'heatwaves',\n",
    "                           'import_metadata': 'yes',\n",
    "                           'imp_dim': 'time', \n",
    "                           'imp_concept_level': 'd',\n",
    "                           'hierarchy': 'oph_base|oph_base|oph_time',\n",
    "                           'description': 'Max Temp climatological mean', \n",
    "                           'input': '/home/ophidia/notebooks/climatological_mean.nc'},\n",
    "                dependencies={t1:''})\n",
    "t3 = e6.newTask(name=\"Import\",\n",
    "                type=\"ophidia\",\n",
    "                operator='oph_importnc',\n",
    "                arguments={'measure': 'tasmax',\n",
    "                           'container': 'heatwaves',\n",
    "                           'import_metadata': 'yes',\n",
    "                           'imp_dim': 'time', \n",
    "                           'imp_concept_level': 'd',\n",
    "                           'hierarchy': 'oph_base|oph_base|oph_time',\n",
    "                           'description': 'Max Temp year 2100', \n",
    "                           'input': '/home/ophidia/notebooks/tasmax_day_CMCC-CESM_rcp85_r1i1p1_21000101-21001231.nc'},\n",
    "                dependencies={t1:''})\n",
    "t4 = e6.newTask(name=\"Intercube\",\n",
    "                type=\"ophidia\",\n",
    "                operator='oph_intercube', \n",
    "                arguments={'operation': 'sub', \n",
    "                           'description': 'Result from intercube'},\n",
    "                dependencies={t2:'cube2', t3:'cube'})\n",
    "t5 = e6.newTask(name=\"Apply\",\n",
    "                type=\"ophidia\",\n",
    "                operator='oph_apply',\n",
    "                arguments={'query': \"oph_predicate('OPH_INT','OPH_INT',oph_sequence('OPH_INT','OPH_INT', oph_predicate('OPH_FLOAT','OPH_INT',oph_predicate('OPH_FLOAT','OPH_FLOAT',measure,'x-100','>0','0','x'),'x-5','>0','1','0'), 'length', 'yes'),'x-5','>0','x','0')\",\n",
    "                           'description': 'Heat Wave Duration cube'},\n",
    "                dependencies={t4:'cube'})\n",
    "t6 = e6.newTask(name=\"Reduce\",\n",
    "                type=\"ophidia\",\n",
    "                operator='oph_reduce', \n",
    "                arguments={'operation': 'max', \n",
    "                           'dim': 'time', \n",
    "                           'description': 'Heat Wave Duration Index cube'}, \n",
    "                dependencies={t5:'cube'})\n",
    "t7 = e6.newTask(name=\"Apply for HWN\",\n",
    "                type=\"ophidia\",\n",
    "                operator='oph_apply', \n",
    "                arguments={'query': \"oph_predicate('OPH_INT','OPH_INT',measure,'x','>0','1','0')\",\n",
    "                           'description': 'Apply for HWN cube'},\n",
    "                dependencies={t5:'cube'})\n",
    "t8 = e6.newTask(name=\"Reduce for HWN\",\n",
    "                type=\"ophidia\",\n",
    "                operator='oph_reduce2', \n",
    "                arguments={'operation': 'sum',\n",
    "                           'dim': 'time', \n",
    "                           'description': 'Heat Wave Number cube'},\n",
    "                dependencies={t7:'cube'})\n",
    "t9 = e6.newTask(name=\"Reduce for HWF\",\n",
    "                type=\"ophidia\",\n",
    "                operator='oph_reduce2', \n",
    "                arguments={'operation': 'sum',\n",
    "                           'dim': 'time', \n",
    "                           'description': 'Reduce for HWF cube'},\n",
    "                dependencies={t5:'cube'})\n",
    "t10 = e6.newTask(name=\"Apply for HWF\",\n",
    "                type=\"ophidia\",\n",
    "                operator='oph_apply', \n",
    "                arguments={'query': \"oph_mul_scalar('OPH_INT', 'OPH_FLOAT', measure,\"+ str(1/365) +\")\",\n",
    "                           'description': 'Heat Wave Frequency cube'},\n",
    "                dependencies={t9:'cube'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the workflow..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w6 = Workflow(e6)\n",
    "w6.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and check the experiment execution progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w6.monitor(frequency=1, iterative=True, visual_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the cubes created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.list(level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get PID of 'Heat Wave Duration Index cube'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HWDI = cube.Cube(pid='http://127.0.0.1/ophidia/.../...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a map of HWDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "from cartopy.mpl.geoaxes import GeoAxes\n",
    "from cartopy.util import add_cyclic_point\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "fig = plt.figure(figsize=(15, 6), dpi=100)\n",
    "\n",
    "#Add Geo axes to the figure with the specified projection (PlateCarree)\n",
    "projection = ccrs.PlateCarree()\n",
    "ax = plt.axes(projection=projection)\n",
    "\n",
    "#Draw coastline and gridlines\n",
    "ax.coastlines()\n",
    "\n",
    "gl = ax.gridlines(crs=projection, draw_labels=True, linewidth=1, color='black', alpha=0.9, linestyle=':')\n",
    "gl.xlabels_top = False\n",
    "gl.ylabels_right = False\n",
    "\n",
    "data = HWDI.export_array(show_time='yes')\n",
    "lat = data['dimension'][0]['values'][ : ]\n",
    "lon = data['dimension'][1]['values'][ : ]\n",
    "var = data['measure'][0]['values'][ : ]\n",
    "var = np.reshape(var, (len(lat), len(lon)))\n",
    "\n",
    "#Wraparound points in longitude\n",
    "var_cyclic, lon_cyclic = add_cyclic_point(var, coord=np.asarray(lon))\n",
    "x, y = np.meshgrid(lon_cyclic,lat)\n",
    "\n",
    "#Define color levels for color bar\n",
    "levStep = (np.nanmax(var)-np.nanmin(var))/20\n",
    "clevs = np.arange(np.nanmin(var),np.nanmax(var)+levStep,levStep)\n",
    "\n",
    "#Set filled contour plot\n",
    "cnplot = ax.contourf(x, y, var_cyclic, clevs, transform=projection,cmap=plt.cm.Oranges)\n",
    "plt.colorbar(cnplot,ax=ax)\n",
    "\n",
    "ax.set_aspect('auto', adjustable=None)\n",
    "\n",
    "plt.title('HWDI (year 2100)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border-top:1px solid #0000FF\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empty the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.deletecontainer(container=\"heatwaves\",force='yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The virtual file system should now be \\\"clean\\\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
