{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic workflow composition with Ophidia\n",
    "\n",
    "This notebook provides a very simple example of how to build an Ophidia workflow with the JSON schema and submit it by reusing some of the concepts shown during the demo. \n",
    "\n",
    "A workflow allows users to code a set of data processing and analytics steps into reusable documents. Moreover, the workflow manager can optimize the workflow execution to run concurrently independent operations (tasks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Preliminary steps\n",
    "\n",
    "As first step, let's connect to the Ophidia Server. The **PyOphidia** module will be used to submit the workflow to the Ophidia workflow manager for the computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from PyOphidia import cube,client\n",
    "cube.Cube.setclient(read_env=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will be using a single NetCDF files produced by the CMCC-CESM model and related to the tasmin variable for the period 2096-2100. The file is located under ```/home/ophidia/notebooks/```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "glob.glob('/home/ophidia/notebooks/tasmin_*.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Workflow composition\n",
    "\n",
    "The workflow in this example consists of four sequential tasks:\n",
    "\n",
    "1. Creation of a container for the datacubes \n",
    "2. Import of the NetCDF file\n",
    "3. Extraction of a multi-dimensional subset\n",
    "4. Computation of the average over the time series\n",
    "\n",
    "The overall workflow structure is the following:\n",
    "    \n",
    "<img src=\"../imgs/Example_workflow.svg\" alt=\"Example_workflow\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define global arguments\n",
    "\n",
    "First of all we need to define the global arguments of the workflow and in particular its ```name``` and its ```execution_mode``` as show in the JSON code.\n",
    "\n",
    "```sync``` means that when submitting the workflow the function will block until the execution is completed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_workflow = \"\"\"{\n",
    "        \"name\": \"Example workflow\",\n",
    "        \"author\": \"CMCC\",\n",
    "        \"abstract\": \"Example workflow\",\n",
    "        \"exec_mode\": \"sync\",\n",
    "        \"ncores\": \"1\",\n",
    "        \"cwd\": \"/\",\n",
    "        \"tasks\":\n",
    "        [\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the tasks\n",
    "\n",
    "The first task of the workflow is the *oph_createcontainer* operator to create an empty container to organize all the datacubes imported and produced during the workflow execution.\n",
    "\n",
    "The ```on_error``` argument is set to ```skip``` in order to simply skip the task in case of error; for example if a container of the same name already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_workflow += \"\"\"\n",
    "                {\n",
    "                        \"name\": \"Create container\",\n",
    "                        \"operator\": \"oph_createcontainer\",\n",
    "                        \"arguments\": \n",
    "                        [\n",
    "                                \"container=example\",\n",
    "                                \"dim=lat|lon|time\",\n",
    "                                \"dim_type=double|double|double\",\n",
    "                                \"hierarchy=oph_base|oph_base|oph_time\"\n",
    "                        ],\n",
    "                        \"on_error\": \"skip\"\n",
    "                },\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second task defined is the *oph_importnc* operator to load data from a NetCDF into an Ophidia datacube.\n",
    "\n",
    "In this case we use the ```$1``` and ```$2``` variables to define the operator ```src_path``` and ```measure``` arguments at runtime. The same could be applied to anyother argument.\n",
    "\n",
    "A flow dependency with respect to the previous task is set, in order to run the data import only after the container has been created. \n",
    "\n",
    "The ```on_error``` argument is set to ```break``` so that the whole workflow stops if the tasks fails. This is the default value so it does not need to be specified for each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_workflow += \"\"\"\n",
    "                {\n",
    "                        \"name\": \"Import\",\n",
    "                        \"operator\": \"oph_importnc\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"src_path=$1\",                            \n",
    "                                \"measure=$2\",\n",
    "                                \"container=example\",\n",
    "                                \"imp_dim=time\",\n",
    "                                \"description=Imported cube\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Create container\"}\n",
    "                        ],\n",
    "                        \"on_error\": \"break\"\n",
    "                },   \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third task defines a *oph_subset* task to extract a portion of the datacube from the imported one.\n",
    "\n",
    "A data dependency of ```\"type\": \"single\"``` on the import task is defined in order to use the output cube from the previous task as input to this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_workflow += \"\"\"\n",
    "                {\n",
    "                        \"name\": \"Subset\",\n",
    "                        \"operator\": \"oph_subset\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"subset_filter=30:70|-20:40\",\n",
    "                                \"subset_dims=lat|lon\",\n",
    "                                \"subset_type=coord\",\n",
    "                                \"description=Subsetted cube\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Import\", \"type\": \"single\" }\n",
    "                        ]\n",
    "                },\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally a *oph_reduce* task is added to compute the average over the time series of the *Subsetted Cube*.\n",
    "\n",
    "Again a data dependency of ```\"type\": \"single\"``` is defined in order to use the output cube from the previous task as input to this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_workflow += \"\"\"\n",
    "                {\n",
    "                        \"name\": \"Reduce\",\n",
    "                        \"operator\": \"oph_reduce\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"operation=avg\",\n",
    "                                \"description=Reduced cube\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Subset\", \"type\": \"single\" }\n",
    "                        ]\n",
    "                }                \n",
    "        ]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Submit the workflow\n",
    "\n",
    "Now the ```example_workflow``` workflow can be submitted to the Ophidia workflow manager, which in turn will take care of the execution of the various task. This can be done with the ```cube.Cube.client.wsubmit``` Python line.\n",
    "\n",
    "At this stage we need to set the values for the ```$1``` and ```$2``` variables defined for the *Import* task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.client.wsubmit(example_workflow, \n",
    "                         \"/home/ophidia/notebooks/tasmin_day_CMCC-CESM_rcp85_r1i1p1_20960101-21001231.nc\", \n",
    "                         \"tasmin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returns on the top part of the output the status of the general workflow. It the execution was completed successfully this should be ```OPH_STATUS_COMPLETED```.\n",
    "\n",
    "The following table reports a summary about the total number of task from the workflow executed and those completed with success.\n",
    "\n",
    "The third table report the status of each of the tasks composing the workflow.\n",
    "\n",
    "Finally the execution time is also reported at the end of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the three datacubes create with the ```list``` PyOphidia function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.list(level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Final remarks\n",
    "\n",
    "You've completed your first workflow with Ophidia. If you would like to get more technical info about the workflow features provided by Ophidia check the documentation [**http://ophidia.cmcc.it/documentation/users/workflow/index.html**](http://ophidia.cmcc.it/documentation/users/workflow/index.html).  \n",
    "\n",
    "Before moving to the other hands-on notebooks, clear the cube space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.deletecontainer(container=\"example\",force='yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now move to the last hands-on notebook [**2-Summer_days_workflow**](2-Summer_days_workflow.ipynb).\n",
    "\n",
    "If you are interested in running other workflows examples check the [**Examples**](../Examples/) folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
