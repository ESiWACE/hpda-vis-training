{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical examples of simple workflow building and integration with PyOphidia module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will show the basic usage and some advanced features (dependency specification, iterative and parallel interfaces, selection interface, task error handling) related to the Ophidia workflows.\n",
    "\n",
    "We will be using **PyOphidia** module and in particular the ```wsubmit``` method to submit a workflow to the Ophidia server.\n",
    "\n",
    "First of all, import **PyOphidia** module and setup a connection to the **Ophidia Server**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from PyOphidia import cube,client\n",
    "cube.Cube.setclient(read_env=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic workflow: import + subset + export\n",
    "\n",
    "The following JSON object is an example of workflow with 3 tasks: \n",
    "- **one independent task**: *Import* to import a NetCDF file into a datacube\n",
    "- **two dependent tasks**: *Subset* and *Export* to perform a subsetting operation along the dimensions of the  datacube and export the result into a new NetCDF file.\n",
    "\n",
    "\n",
    "<img src=\"imgs/1_Basic_workflow.svg\" alt=\"basic_workflow\" width=\"100\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = \"\"\"{\n",
    "        \"name\": \"Basic workflow\",\n",
    "        \"author\": \"CMCC\",\n",
    "        \"abstract\": \"Perform some basics operations using workflows\",\n",
    "        \"exec_mode\": \"sync\",\n",
    "        \"ncores\": \"2\",\n",
    "        \"on_exit\": \"oph_delete\",\n",
    "        \"cwd\": \"/\",\n",
    "        \"tasks\":\n",
    "        [\n",
    "                {\n",
    "                        \"name\": \"Import\",\n",
    "                        \"operator\": \"oph_importnc\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"src_path=/home/ophidia/notebooks/$1\",\n",
    "                                \"measure=$2\",\n",
    "                                \"import_metadata=yes\",\n",
    "                                \"imp_dim=time\",\n",
    "                                \"imp_concept_level=d\",\n",
    "                                \"vocabulary=CF\",\n",
    "                                \"hierarchy=oph_base|oph_base|oph_time\",\n",
    "                                \"description=Max Temp\"\n",
    "                        ]\n",
    "                },        \n",
    "                {\n",
    "                        \"name\": \"Subset\",\n",
    "                        \"operator\": \"oph_subset\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"subset_filter=JJA\",\n",
    "                                \"subset_dims=time\",\n",
    "                                \"subset_type=coord\",\n",
    "                                \"description=JJA\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Import\", \"type\": \"single\", \"argument\":\"cube\" }\n",
    "                        ]\n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"Export\",\n",
    "                        \"operator\": \"oph_exportnc2\",\n",
    "                        \"arguments\": [\n",
    "                            \"output_name=JJA\",\n",
    "                            \"output_path=/home/ophidia/notebooks/\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                            { \"task\": \"Subset\", \"type\": \"single\"}\n",
    "                        ]\n",
    "                }\n",
    "        ]\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start building the first workflow step by step!\n",
    "\n",
    "First of all, we define several **global attributes**, which include a number of metadata and default parameters values common to all the tasks. \n",
    "Some of these keywords are mandatory:\n",
    "- ```name```: the title of the workflow\n",
    "- ```author```: the author’s name\n",
    "- ```abstract```: a short description of the workflow\n",
    "\n",
    "The parameter ```exec_mode``` specifies the execution mode, synchronous or asynchronous, and it refers to the entire workflow, not to single tasks. In case of synchronous mode the workflow will be executed in a blocking way, so the submitter will have to wait until it will be finished to display the results. If the execution mode is asynchronous, the workflow will be processed in a non-blocking way (like a batch mode), allowing the submitter to immediately take the control and eventually submit other requests. After sending an asynchronous request, the user can get workflow output (if available) by exploiting the **view** command.\n",
    "\n",
    "Since a lot of tasks could be launched in parallel, an important parameter is the number of cores per task (```ncores```) which specifies the default value to be associated with all the workflow’s tasks. This value can be overridden with another one tailored to a task with a different behaviour.\n",
    "\n",
    "By using the ```on_exit``` parameter the user can select the cubes that will be dropped out when a workflow ends.\n",
    "The default value of ```on_exit``` can be set as global attribute. Admitted values are:\n",
    "- oph_delete: remove the output cube\n",
    "- oph_deletecontainer: remove the output container (valid only for OPH_CREATECONTAINER)\n",
    "- nop: no operation has to be applied (default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = \"\"\"{\n",
    "        \"name\": \"Basic workflow\",\n",
    "        \"author\": \"CMCC\",\n",
    "        \"abstract\": \"Perform some basics operations using workflows\",\n",
    "        \"exec_mode\": \"sync\",\n",
    "        \"ncores\": \"2\",\n",
    "        \"on_exit\": \"oph_delete\",\n",
    "        \"cwd\": \"/\",\n",
    "        \"tasks\":\n",
    "        [\n",
    "\"\"\"        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each task is uniquely identified within the workflow by its ```name``` and it is related to a specific Ophidia Operator set as ```operator```. According to that operator, the user can optionally insert a JSON array of key-value pairs (```arguments```) in order to call the operator with the appropriate arguments.\n",
    "\n",
    "The first task of the workflow is related to the *oph_importnc* operator. According to the operator specification, we need to specify the mandatory arguments: *src_path* and *measure*. In this example, the two arguments values, ```$1``` and ```$2```, will be replaced with the workflow input parameters before sending the request to the Ophidia Server.\n",
    "\n",
    "In addition, we can specify other arguments such as:\n",
    "- *import_metadata=yes* to import also metadata from the input NetCDF file\n",
    "- *imp_dim=time* to arrange data in order to operate on time series\n",
    "- *imp_concept_level=d* to set the concept level to *day*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow += \"\"\"\n",
    "\n",
    "{\n",
    "                        \"name\": \"Import\",\n",
    "                        \"operator\": \"oph_importnc\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"src_path=/home/ophidia/notebooks/$1\",\n",
    "                                \"measure=$2\",\n",
    "                                \"import_metadata=yes\",\n",
    "                                \"imp_dim=time\",\n",
    "                                \"imp_concept_level=d\",\n",
    "                                \"vocabulary=CF\",\n",
    "                                \"hierarchy=oph_base|oph_base|oph_time\",\n",
    "                                \"description=Max Temp\"\n",
    "                        ]\n",
    "                }, \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second task is related to the *oph_subset* operator. \n",
    "\n",
    "For example, we can consider the whole spatial domain and specify a subset only on the time range, as indicated by the *subset_dims* parameter.\n",
    "We can select a particular season by using the corresponding code for the *subset filter* argument:\n",
    " - *DJF* for winter\n",
    " - *MAM* for spring\n",
    " - *JJA* for summer\n",
    " - *SON* for autumn\n",
    "\n",
    "The input cube is the output cube of the previous task (*Import*), so we have to specify a dependency between these two tasks.\n",
    "\n",
    "The dependencies can be specified by a JSON array put in the section ```dependencies``` of the dependent task (child). Each item of this array is a JSON object related to a specific parent task which the child depends on. \n",
    "In general, it reports: the name of the **parent task** (```task```) and the ```type``` of the dependency:\n",
    "- *single*: if the child task exploits only one output of parent task;\n",
    "- *all*: if the child task processes all the outputs of parent task (e.g. dependencies between massive operations);\n",
    "- *embedded*: to specify a simple flow dependency (the child task has to begin only after the parent task has finished), with no dependency on the outputs of the parent task.\n",
    "\n",
    "By default, for the first two options, the ```argument``` parameter of a dependency is set to **cube** so that it can be usually omitted.\n",
    "\n",
    "It is also possible to specify the particular operator argument whose value is depending on the output produced by another task.\n",
    "\n",
    "In the example below, we set\n",
    "- ```task``` equal to the name of the parent task (i.e. Import);\n",
    "- ```type``` equal to ```single``` since the child task will use only one output from the parent task.\n",
    "\n",
    "We could omit ```argument``` since by default the ```cube``` parameter of the *oph_subset* operator will be set to the PID of the cube imported in the *Import* task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow+= \"\"\"\n",
    "\n",
    "{\n",
    "                        \"name\": \"Subset\",\n",
    "                        \"operator\": \"oph_subset\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"subset_filter=JJA\",\n",
    "                                \"subset_dims=time\",\n",
    "                                \"subset_type=coord\",\n",
    "                                \"description=JJA\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Import\", \"type\": \"single\", \"argument\": \"cube\" }\n",
    "                        ]\n",
    "                },\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can export the subsetted cube by using the *oph_exportnc2* operator.\n",
    "\n",
    "In this case we have to:\n",
    "- provide arguments for the oph_exportnc2 operator: *output_name* and *output_path*\n",
    "- set a ```single``` dependency from the previous (i.e. Subset) task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow+= \"\"\"\n",
    "\n",
    "{\n",
    "                        \"name\": \"Export\",\n",
    "                        \"operator\": \"oph_exportnc2\",\n",
    "                        \"arguments\": [\n",
    "                            \"output_name=JJA\",\n",
    "                            \"output_path=/home/ophidia/notebooks/\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                            { \"task\": \"Subset\", \"type\": \"single\"}\n",
    "                        ]\n",
    "                }\n",
    "        ]\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JSON object related to our first workflow is ready!\n",
    "\n",
    "We have to define the workflow input arguments:\n",
    "- *NetCDF filename* under the */home/ophidia/notebooks/* folder\n",
    "- *variable* to be imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=\"tasmax_day_CMCC-CESM_rcp85_r1i1p1_20960101-21001231.nc\"\n",
    "variable=\"tasmax\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To submit the workflow within the notebook we can use the **wsubmit** PyOphidia method, providing:\n",
    "- the workflow name\n",
    "- the input parameters\n",
    "\n",
    "In our first example the workflow has two parameters  (```$1``` and ```$2```):\n",
    "- the source path used in the oph_importnc operator to load the NetCDF file\n",
    "- the variable name use to set the measure argument in the import operator\n",
    "\n",
    "The output of the workflow is a report with the final status of each task. By default three objects are included:\n",
    " - a text object **Workflow Status**, which reports the workflow status;\n",
    " - a table object **Workflow Progress**, which reports the total number of tasks and the number of completed tasks;\n",
    " - a table object **Workflow Task List**, which reports a list with some information about each task: job identifier, Marker ID, Task Name, Task Type (simple or massive) and Task Status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.client.wsubmit(workflow, file, variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check exported file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -lh /home/ophidia/notebooks | grep \"\\.nc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cube workspace is already empty (we have no datacubes) because we've used ```\"on_exit\": \"oph_delete\"``` as workflow global attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.list(level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just need to remove the container automatically created by the *oph_importnc* operator. By default, the container name is equal to the name of the imported file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.deletecontainer(container=file,force='yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Workflows: iterative and parallel interfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now consider a slightly more complex workflow, in which we are going to:\n",
    "- import and subset multiple NetCDF files\n",
    "- merge all the subsetted datacubes\n",
    "- perform a reduction (avg, max, min, ...) operation\n",
    "- export the output datacube\n",
    "\n",
    "<img src=\"imgs/2_Iterative_parallel.svg\" alt=\"iterative_parallel\" width=\"800\">\n",
    "\n",
    "As input files, we can use the daily NetCDF files produced by the CMCC-CESM model and related to the *tasmax* variable for the years 2096-2100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /home/ophidia/notebooks | grep \"tasmax\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JSON object associated to the workflow consists of several tasks. In addition to the Ophidia Data Import/Export & analysis operators, we are going to exploit the ```oph_for``` and ```oph_endfor``` flow control operators to implement a **for** loop. \n",
    "\n",
    "Unlike other Ophidia operators, these do not operate on data or metadata, but can be adopted to set particular flow control rules for the Workflow manager. In particular, the operators are used to begin/end a sub-section that has to be executed several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_loop = \"\"\"{\n",
    "        \"name\": \"Loop operations\",\n",
    "        \"author\": \"CMCC\",\n",
    "        \"abstract\": \"Perform some basics operations using workflows\",\n",
    "        \"exec_mode\": \"sync\",\n",
    "        \"ncores\": \"1\",\n",
    "        \"on_exit\": \"oph_delete\",\n",
    "        \"cwd\": \"/\",\n",
    "        \"tasks\":\n",
    "        [\n",
    "                {\n",
    "                        \"name\": \"Create container\",\n",
    "                        \"operator\": \"oph_createcontainer\",\n",
    "                        \"on_error\": \"skip\",\n",
    "                        \"arguments\": \n",
    "                        [\n",
    "                                \"container=workflow\",\n",
    "                                \"dim=lat|lon|time\",\n",
    "                                \"dim_type=double|double|double\",\n",
    "                                \"hierarchy=oph_base|oph_base|oph_time\"\n",
    "                        ]\n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"Start loop\",\n",
    "                        \"operator\": \"oph_for\",\n",
    "                        \"arguments\": \n",
    "                        [\n",
    "                                \"key=year\",\n",
    "                                \"values=2096|2097|2098|2099|2100\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Create container\"}\n",
    "                        ]\n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"Import\",\n",
    "                        \"operator\": \"oph_importnc\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"src_path=/home/ophidia/notebooks/tasmax_day_CMCC-CESM_rcp85_r1i1p1_@{year}0101-@{year}1231.nc\",                           \n",
    "                                \"measure=tasmax\",\n",
    "                                \"container=workflow\",\n",
    "                                \"import_metadata=yes\",\n",
    "                                \"imp_dim=time\",\n",
    "                                \"imp_concept_level=d\",\n",
    "                                \"vocabulary=CF\",\n",
    "                                \"hierarchy=oph_base|oph_base|oph_time\",\n",
    "                                \"description=Max Temp @{year}\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Start loop\"}\n",
    "                        ]\n",
    "                },   \n",
    "                {\n",
    "                        \"name\": \"Subset\",\n",
    "                        \"operator\": \"oph_subset\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"subset_filter=JJA\",\n",
    "                                \"subset_dims=time\",\n",
    "                                \"subset_type=coord\",\n",
    "                                \"description=JJA @{year}\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Import\", \"type\": \"single\" }\n",
    "                        ]\n",
    "                        \n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"End loop year\",\n",
    "                        \"operator\": \"oph_endfor\",\n",
    "                        \"arguments\": [],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Subset\", \"type\": \"all\" }\n",
    "                        ]\n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"Merge\",\n",
    "                        \"operator\": \"oph_mergecubes\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"description=Merged cube\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"End loop year\", \"type\": \"all\", \"argument\": \"cubes\" }\n",
    "                        ]\n",
    "                        \n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"Reduce\",\n",
    "                    \"operator\": \"oph_reduce\",\n",
    "                    \"arguments\": [\n",
    "                        \"operation=avg\",\n",
    "                        \"description=Reduced cube\",\n",
    "                        \"dim=time\"\n",
    "                    ],\n",
    "                    \"dependencies\": [\n",
    "                        { \"task\": \"Merge\", \"type\": \"single\" }\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"Export\",\n",
    "                        \"operator\": \"oph_exportnc2\",\n",
    "                        \"arguments\": [\n",
    "                            \"output_name=avg_JJA\",\n",
    "                            \"output_path=/home/ophidia/notebooks/\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                            { \"task\": \"Reduce\", \"type\": \"single\"}\n",
    "                        ]\n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"Delete container\",\n",
    "                        \"operator\": \"oph_deletecontainer\",\n",
    "                        \"arguments\": [\n",
    "                                \"container=workflow\",\n",
    "                                \"force=yes\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Export\", \"type\": \"embedded\" }\n",
    "                        ]\n",
    "                }\n",
    "                \n",
    "        ]\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the workflow step by step.\n",
    "\n",
    "In the following cell we define some global attributes as in the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_loop = \"\"\"{\n",
    "        \"name\": \"Loop operations\",\n",
    "        \"author\": \"CMCC\",\n",
    "        \"abstract\": \"Perform some basics operations using workflows\",\n",
    "        \"exec_mode\": \"sync\",\n",
    "        \"ncores\": \"1\",\n",
    "        \"on_exit\": \"oph_delete\",\n",
    "        \"cwd\": \"/\",\n",
    "        \"tasks\":\n",
    "        [\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a new container**\n",
    "\n",
    "This is the first task, so it has no dependencies. We just have to provide the proper arguments to the *oph_createcontainer* operator:\n",
    "- the container name\n",
    "- the name and the type of the dimensions allowed\n",
    "- the concept hierarchy name of the dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_loop += \"\"\"\n",
    "{\n",
    "                        \"name\": \"Create container\",\n",
    "                        \"operator\": \"oph_createcontainer\",\n",
    "                        \"arguments\": \n",
    "                        [\n",
    "                                \"container=workflow\",\n",
    "                                \"dim=lat|lon|time\",\n",
    "                                \"dim_type=double|double|double\",\n",
    "                                \"hierarchy=oph_base|oph_base|oph_time\"\n",
    "                        ]\n",
    "                },\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FOR statement**\n",
    "\n",
    "The OPH_FOR operator is used to configure the iterative block and, in particular, to set the number N of loops to be executed. By this aim, we have to provide an ordered list of N labels to be assigned to cycles in order to distinguish one cycle from another one. The list is assigned to the ```values``` parameter, separating each value by |. \n",
    "\n",
    "In our example, we provide a list of years in order to import the corresponding NetCDF file in the next task.\n",
    "\n",
    "A name has to be associated to the list values by setting the ```key``` parameter (e.g. ```year```), which is used in the inner tasks in the form **@{key_name}** to access the current value of the counter/label. \n",
    "\n",
    "Finally, we define a simple flow dependency (```type=embedded```), since this task has to begin only after the previous \"Create container\" task has finished, but no data is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_loop += \"\"\"\n",
    "{\n",
    "                        \"name\": \"Start loop\",\n",
    "                        \"operator\": \"oph_for\",\n",
    "                        \"arguments\": \n",
    "                        [\n",
    "                                \"key=year\",\n",
    "                                \"values=2096|2097|2098|2099|2100\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Create container\", \"type\":\"embedded\"}\n",
    "                        ]\n",
    "                },\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import and subset multiple datacubes in parallel**\n",
    "\n",
    "The two inner tasks to be repeated (import and subset) have to depend on OPH_FOR directly or indirectly, namely they depend on other tasks in the iterative block. \n",
    "\n",
    "Setting the parameters of these tasks the user is able to exploit the value of the label associated with current iteration. \n",
    "\n",
    "*IMPORT task*\n",
    "\n",
    "The **src_path** as well the **description** parameters in the *oph_importnc* operator are defined in a parametrized way to get the current value of the **year** key for each iteration.\n",
    "\n",
    "This task has a simple flow dependency from the \"Start loop\" task in order to start after this task and retrieve the right value of the label associated with current iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_loop += \"\"\"\n",
    "{\n",
    "                        \"name\": \"Import\",\n",
    "                        \"operator\": \"oph_importnc\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"src_path=/home/ophidia/notebooks/tasmax_day_CMCC-CESM_rcp85_r1i1p1_@{year}0101-@{year}1231.nc\",                           \n",
    "                                \"measure=tasmax\",\n",
    "                                \"container=workflow\",\n",
    "                                \"import_metadata=yes\",\n",
    "                                \"imp_dim=time\",\n",
    "                                \"imp_concept_level=d\",\n",
    "                                \"vocabulary=CF\",\n",
    "                                \"hierarchy=oph_base|oph_base|oph_time\",\n",
    "                                \"description=Max Temp @{year}\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Start loop\", \"type\":\"embedded\"}\n",
    "                        ]\n",
    "                }, \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*SUBSET task*\n",
    "\n",
    "This task has a *single* dependency from the \"Import\" task since each subset operation has to be performed on the corresponding datacube imported at the previous import step, so the output from the *Import* task is the input for the *Subset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_loop += \"\"\"\n",
    "{\n",
    "                        \"name\": \"Subset\",\n",
    "                        \"operator\": \"oph_subset\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"subset_filter=JJA\",\n",
    "                                \"subset_dims=time\",\n",
    "                                \"subset_type=coord\",\n",
    "                                \"description=JJA @{year}\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Import\", \"type\": \"single\" }\n",
    "                        ]\n",
    "                        \n",
    "                },\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End loop**\n",
    "\n",
    "The OPH_ENDFOR operator ends an iterative block, has no arguments and depends on the inner tasks.\n",
    "\n",
    "In our example, it depends on the \"Subset\" task and the dependency type is **all**. In this way, it can gather PIDs of all cubes generated by the (subset) inner task and transfer them to next tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_loop += \"\"\"\n",
    "{\n",
    "                        \"name\": \"End loop\",\n",
    "                        \"operator\": \"oph_endfor\",\n",
    "                        \"arguments\": [],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Subset\", \"type\": \"all\" }\n",
    "                        ]\n",
    "                },\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge all the subsetted datacubes into a single datacube**\n",
    "\n",
    "All the subsetted datacubes can be now merged into a single datacube by using the **oph_mergecubes** operator: the resulting datacube will contain the JJA subset for each of the imported years. \n",
    "\n",
    "As for the previous task, we need to specify an **all** dependency to get all the datacubes PIDs from the previous task. In addition, we have to set the ```argument``` parameter to ```cubes``` so that the value of the *cubes* parameter for the *oph_mergecubes* operator will be set to a list of pipe-separated PIDs retrieved from the \"End loop\" task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_loop += \"\"\"\n",
    "{\n",
    "                        \"name\": \"Merge\",\n",
    "                        \"operator\": \"oph_mergecubes\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"description=Merged cube\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"End loop\", \"type\": \"all\", \"argument\": \"cubes\" }\n",
    "                        ]\n",
    "                        \n",
    "                },\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform a reduction operation**\n",
    "\n",
    "Starting from the merged datacube, we can perform a reduction operation with respect to the implicit dimension (time).\n",
    "\n",
    "We just need to define a *single* dependency between the **Reduce** task and the previous **Merge** task.\n",
    "\n",
    "The reduced cube will contain the average value for the tasmax variable over the 2011-2013 JJA period for each point in the spatial domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_loop += \"\"\"\n",
    "{\n",
    "                    \"name\": \"Reduce\",\n",
    "                    \"operator\": \"oph_reduce\",\n",
    "                    \"arguments\": [\n",
    "                        \"operation=avg\",\n",
    "                        \"description=Reduced cube\",\n",
    "                        \"dim=time\"\n",
    "                    ],\n",
    "                    \"dependencies\": [\n",
    "                        { \"task\": \"Merge\", \"type\": \"single\" }\n",
    "                    ]\n",
    "                },\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export the averaged datacube**\n",
    "\n",
    "In a similar way, we can define an *\\\"Export\\\"* task that depends on the *\\\"Reduce\\\"* task to export data into a single NetCDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_loop += \"\"\"\n",
    "{\n",
    "                        \"name\": \"Export\",\n",
    "                        \"operator\": \"oph_exportnc2\",\n",
    "                        \"arguments\": [\n",
    "                            \"output_name=avg_JJA\",\n",
    "                            \"output_path=/home/ophidia/notebooks/\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                            { \"task\": \"Reduce\", \"type\": \"single\"}\n",
    "                        ]\n",
    "                },\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Empty workspace**\n",
    "\n",
    "Finally, we can remove the container created by the first task of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_loop += \"\"\"\n",
    "{\n",
    "                        \"name\": \"Delete container\",\n",
    "                        \"operator\": \"oph_deletecontainer\",\n",
    "                        \"arguments\": [\n",
    "                                \"container=workflow\",\n",
    "                                \"force=yes\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Export\", \"type\": \"embedded\" }\n",
    "                        ]\n",
    "                }\n",
    "                \n",
    "        ]\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run workflow**\n",
    "\n",
    "We can submit the workflow using the **wsubmit** PyOphidia method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.client.wsubmit(workflow_loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check exported file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -lh /home/ophidia/notebooks/ | grep \"\\.nc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel loop statement\n",
    "\n",
    "The OPH_FOR operator used to configure the iterative block can also be executed in parallel to speed up the execution, if there is no depedency between the iteration branches.\n",
    "\n",
    "In this case we set ```parallel```argument to ```yes``` for parallel processing.\n",
    "\n",
    "If the option is enabled for a OPH_FOR, the engine, before executing the workflow, transforms it into an equivalent version in which iterative blocks are expanded into N independent sub-workflows, where N is the number of initial iterations. The new workflow is then executed taking into account the usual rules based on task dependencies.\n",
    "\n",
    "For example in the prevous *oph_for* definition\n",
    "\n",
    "```\n",
    "{\n",
    "    \"name\": \"Start loop\",\n",
    "    \"operator\": \"oph_for\",\n",
    "    \"arguments\": \n",
    "    [\n",
    "        \"key=year\",\n",
    "        \"values=2096|2097|2098|2099|2100\",\n",
    "        \"parallel=yes\"\n",
    "    ],\n",
    "    \"dependencies\": [\n",
    "        { \"task\": \"Create container\", \"type\":\"embedded\"}\n",
    "    ]\n",
    "},\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The resulting workflow with parallel loop is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_loop_parallel = \"\"\"{\n",
    "        \"name\": \"Loop operations\",\n",
    "        \"author\": \"CMCC\",\n",
    "        \"abstract\": \"Perform some basics operations using workflows\",\n",
    "        \"exec_mode\": \"sync\",\n",
    "        \"ncores\": \"1\",\n",
    "        \"on_exit\": \"oph_delete\",\n",
    "        \"cwd\": \"/\",\n",
    "        \"tasks\":\n",
    "        [\n",
    "                {\n",
    "                        \"name\": \"Create container\",\n",
    "                        \"operator\": \"oph_createcontainer\",\n",
    "                        \"on_error\": \"skip\",\n",
    "                        \"arguments\": \n",
    "                        [\n",
    "                                \"container=workflow\",\n",
    "                                \"dim=lat|lon|time\",\n",
    "                                \"dim_type=double|double|double\",\n",
    "                                \"hierarchy=oph_base|oph_base|oph_time\"\n",
    "                        ]\n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"Start loop\",\n",
    "                        \"operator\": \"oph_for\",\n",
    "                        \"arguments\": \n",
    "                        [\n",
    "                                \"key=year\",\n",
    "                                \"values=2096|2097|2098|2099|2100\",\n",
    "                                \"parallel=yes\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Create container\"}\n",
    "                        ]\n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"Import\",\n",
    "                        \"operator\": \"oph_importnc\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"src_path=/home/ophidia/notebooks/tasmax_day_CMCC-CESM_rcp85_r1i1p1_@{year}0101-@{year}1231.nc\",                           \n",
    "                                \"measure=tasmax\",\n",
    "                                \"container=workflow\",\n",
    "                                \"import_metadata=yes\",\n",
    "                                \"imp_dim=time\",\n",
    "                                \"imp_concept_level=d\",\n",
    "                                \"vocabulary=CF\",\n",
    "                                \"hierarchy=oph_base|oph_base|oph_time\",\n",
    "                                \"description=Max Temp @{year}\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Start loop\"}\n",
    "                        ]\n",
    "                },   \n",
    "                {\n",
    "                        \"name\": \"Subset\",\n",
    "                        \"operator\": \"oph_subset\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"subset_filter=JJA\",\n",
    "                                \"subset_dims=time\",\n",
    "                                \"subset_type=coord\",\n",
    "                                \"description=JJA @{year}\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Import\", \"type\": \"single\" }\n",
    "                        ]\n",
    "                        \n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"End loop year\",\n",
    "                        \"operator\": \"oph_endfor\",\n",
    "                        \"arguments\": [],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Subset\", \"type\": \"all\" }\n",
    "                        ]\n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"Merge\",\n",
    "                        \"operator\": \"oph_mergecubes\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"description=Merged cube\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"End loop year\", \"type\": \"all\", \"argument\": \"cubes\" }\n",
    "                        ]\n",
    "                        \n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"Reduce\",\n",
    "                    \"operator\": \"oph_reduce\",\n",
    "                    \"arguments\": [\n",
    "                        \"operation=avg\",\n",
    "                        \"description=Reduced cube\",\n",
    "                        \"dim=time\"\n",
    "                    ],\n",
    "                    \"dependencies\": [\n",
    "                        { \"task\": \"Merge\", \"type\": \"single\" }\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"Export\",\n",
    "                        \"operator\": \"oph_exportnc2\",\n",
    "                        \"arguments\": [\n",
    "                            \"output_name=avg_JJA_no_parallel\",\n",
    "                            \"output_path=/home/ophidia/notebooks\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                            { \"task\": \"Reduce\", \"type\": \"single\"}\n",
    "                        ]\n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"Delete container\",\n",
    "                        \"operator\": \"oph_deletecontainer\",\n",
    "                        \"arguments\": [\n",
    "                                \"container=workflow\",\n",
    "                                \"force=yes\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Export\", \"type\": \"embedded\" }\n",
    "                        ]\n",
    "                }\n",
    "                \n",
    "        ]\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run workflow**\n",
    "\n",
    "We can submit the workflow using the **wsubmit** PyOphidia method again.\n",
    "\n",
    "*Note the execution time.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.client.wsubmit(workflow_loop_parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -lh /home/ophidia/notebooks/ | grep \"\\.nc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. Workflows: Selection interface\n",
    "\n",
    "The Selection interface provides further flexibility by enabling the Workflow manager to execute one or more tasks based on boolean conditions that could be checked at run-time and depend on input parameters, data, metadata, etc.\n",
    "\n",
    "The development of the Selection interface involved the design of new Ophidia operators:\n",
    " - OPH_IF\n",
    " - OPH_ELSEIF\n",
    " - OPH_ELSE\n",
    " - OPH_ENDIF\n",
    "\n",
    "Similarly to other flow control operators, they does not process data or metadata directly, but they could be adopted to enable (or to skip) the execution of a set of tasks based on run-time conditions.\n",
    "\n",
    "In the following workflow, we'll consider a selection statement with two selection blocks.\n",
    "\n",
    "<img src=\"imgs/3_Selection_Interface.svg\" alt=\"selection_interface\" width=\"800\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_if = \"\"\"{\n",
    "        \"name\": \"Selection Interface\",\n",
    "        \"author\": \"CMCC\",\n",
    "        \"abstract\": \"Selection statement with two selection blocks\",\n",
    "        \"exec_mode\": \"sync\",\n",
    "        \"ncores\": \"1\",\n",
    "        \"tasks\":\n",
    "        [\n",
    "                {\n",
    "                        \"name\": \"IF block\",\n",
    "                        \"operator\": \"oph_if\",\n",
    "                        \"arguments\": [ \"condition=$1\" ]\n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"Import and subset\",\n",
    "                        \"operator\": \"oph_importnc\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"src_path=/home/ophidia/notebooks/$2\",\n",
    "                                \"measure=$3\",\n",
    "                                \"import_metadata=yes\",\n",
    "                                \"imp_dim=time\",\n",
    "                                \"imp_concept_level=d\",\n",
    "                                \"vocabulary=CF\",\n",
    "                                \"hierarchy=oph_base|oph_base|oph_time\",\n",
    "                                \"description=Max Temp imported and subsetted\",\n",
    "                                \"subset_dims=lat|lon|time\",\n",
    "                                \"subset_filter=$4\",\n",
    "                                \"subset_type=coord\"\n",
    "                        ],\n",
    "                        \"dependencies\":\n",
    "                        [\n",
    "                                { \"task\": \"IF block\" }\n",
    "                        ]\n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"ELSE block\",\n",
    "                        \"operator\": \"oph_else\",\n",
    "                        \"arguments\": [  ],\n",
    "                        \"dependencies\":\n",
    "                        [\n",
    "                                { \"task\": \"IF block\" }\n",
    "                        ]\n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"Import data\",\n",
    "                        \"operator\": \"oph_importnc\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"src_path=/home/ophidia/notebooks/$2\",\n",
    "                                \"measure=$3\",\n",
    "                                \"import_metadata=yes\",\n",
    "                                \"imp_dim=time\",\n",
    "                                \"imp_concept_level=d\",\n",
    "                                \"vocabulary=CF\",\n",
    "                                \"hierarchy=oph_base|oph_base|oph_time\",\n",
    "                                \"description=Max Temp imported\"\n",
    "                        ],\n",
    "                        \"dependencies\":\n",
    "                        [\n",
    "                                { \"task\": \"ELSE block\" }\n",
    "                        ]\n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"Subset data\",\n",
    "                        \"operator\": \"oph_subset\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                               \"subset_dims=lat|lon|time\",\n",
    "                               \"subset_filter=$4\",\n",
    "                               \"subset_type=coord\",\n",
    "                               \"description=Max Temp subsetted\" \n",
    "                        ],\n",
    "                        \"dependencies\":\n",
    "                        [\n",
    "                                { \"task\": \"Import data\",\n",
    "                                  \"type\": \"single\" }\n",
    "                        ]\n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"Selection block end\",\n",
    "                        \"operator\": \"oph_endif\",\n",
    "                        \"arguments\": [ ],\n",
    "                        \"dependencies\":\n",
    "                        [\n",
    "                                { \"task\": \"Subset data\"},\n",
    "                                { \"task\": \"Import and subset\" }\n",
    "                        ]\n",
    "                }\n",
    "        ]\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Workflow global attributes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_if = \"\"\"{\n",
    "        \"name\": \"Selection Interface\",\n",
    "        \"author\": \"CMCC\",\n",
    "        \"abstract\": \"Selection statement with two selection blocks\",\n",
    "        \"exec_mode\": \"sync\",\n",
    "        \"ncores\": \"1\",\n",
    "        \"tasks\":\n",
    "        [\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IF block**\n",
    "\n",
    "The selection interface is used to code two possible implementations of a task that imports data into the Ophidia platform from an external source:\n",
    " 1. import only the subset from the input file\n",
    " 2. import all the dataset and then extract a data subset\n",
    "\n",
    "The actual implementation to be adopted is selected by means of the input parameter ```$1```: a numerical non-zero value for option A, 0 for option B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_if += \"\"\"\n",
    "{\n",
    "                        \"name\": \"IF block\",\n",
    "                        \"operator\": \"oph_if\",\n",
    "                        \"arguments\": [ \"condition=$1\" ]\n",
    "                },\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CASE A: Import only the subset from the input file**\n",
    "\n",
    "In general, the set of tasks belonging to the branch that begins from OPH_IF and ends to OPH_ENDIF is the sub-workflow to be executed in case the condition set for OPH_IF is satisfied.\n",
    "\n",
    "In our example, there is only one task, named *\\\"Import and subset\\\"*, which is related to the *oph_importnc* operator and has a flow dependency from the \"IF block\" task.\n",
    "\n",
    "The *src_path* and the *measure* arguments will be set according to the second and third workflow input arguments (```$2``` and ```$3```).\n",
    "\n",
    "To import only a subset from the input file we have to specify in addition the following parameters:\n",
    "- **subset_dims**: the dimension names used for the subsetting;\n",
    "- **subset_type=coord** so that the filter is considered on dimension values;\n",
    "- **subset_filter**: list of pipe-separated filters associated to each dimension specified in *subset_dims* (set according to the fourth workflow input argument ```$4```).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_if += \"\"\"\n",
    "{\n",
    "                        \"name\": \"Import and subset\",\n",
    "                        \"operator\": \"oph_importnc\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"src_path=/home/ophidia/notebooks/$2\",\n",
    "                                \"measure=$3\",\n",
    "                                \"import_metadata=yes\",\n",
    "                                \"imp_dim=time\",\n",
    "                                \"imp_concept_level=d\",\n",
    "                                \"vocabulary=CF\",\n",
    "                                \"hierarchy=oph_base|oph_base|oph_time\",\n",
    "                                \"description=Max Temp imported and subsetted\",\n",
    "                                \"subset_dims=lat|lon|time\",\n",
    "                                \"subset_filter=$4\",\n",
    "                                \"subset_type=coord\"\n",
    "                        ],\n",
    "                        \"dependencies\":\n",
    "                        [\n",
    "                                { \"task\": \"IF block\" }\n",
    "                        ]\n",
    "                },\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ELSE block**\n",
    "\n",
    "The task with the OPH_ELSE operator has to be a child of the task with the OPH_IF operator. It has no arguments: it simply starts the last sub-block of a selection block \"if\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_if += \"\"\"\n",
    "{\n",
    "                        \"name\": \"ELSE block\",\n",
    "                        \"operator\": \"oph_else\",\n",
    "                        \"arguments\": [  ],\n",
    "                        \"dependencies\":\n",
    "                        [\n",
    "                                { \"task\": \"IF block\" }\n",
    "                        ]\n",
    "                },\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CASE B: import all the dataset and then extract a data subset**\n",
    "\n",
    "The set of tasks belonging to the branch that begins from OPH_ELSE and ends to OPH_ENDIF is the sub-workflow to be executed in case the condition set for OPH_IF is NOT satisfied.\n",
    "\n",
    "In our example, we have two tasks:\n",
    "- the first one, **\\\"Import data\\\"**, is related to the *oph_importnc* operator and is child of the task with the \"OPH_ELSE\" operator.\n",
    "- the second one,**\\\"Subset data\\\"**, is related to the *oph_subset* operator and has a ```single``` dependency from the \"Import data\" task since the input datacube to be subsetted is the datacube generated from the import task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_if += \"\"\"\n",
    "{\n",
    "                        \"name\": \"Import data\",\n",
    "                        \"operator\": \"oph_importnc\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"src_path=/home/ophidia/notebooks/$2\",\n",
    "                                \"measure=$3\",\n",
    "                                \"import_metadata=yes\",\n",
    "                                \"imp_dim=time\",\n",
    "                                \"imp_concept_level=d\",\n",
    "                                \"vocabulary=CF\",\n",
    "                                \"hierarchy=oph_base|oph_base|oph_time\",\n",
    "                                \"description=Max Temp imported\"\n",
    "                        ],\n",
    "                        \"dependencies\":\n",
    "                        [\n",
    "                                { \"task\": \"ELSE block\" }\n",
    "                        ]\n",
    "                },\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_if += \"\"\"\n",
    "{\n",
    "                        \"name\": \"Subset data\",\n",
    "                        \"operator\": \"oph_subset\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                               \"subset_dims=lat|lon|time\",\n",
    "                               \"subset_filter=$4\",\n",
    "                               \"subset_type=coord\",\n",
    "                               \"description=Max Temp subsetted\" \n",
    "                        ],\n",
    "                        \"dependencies\":\n",
    "                        [\n",
    "                                { \"task\": \"Import data\",\n",
    "                                  \"type\": \"single\" }\n",
    "                        ]\n",
    "                },\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ENDIF block**\n",
    "\n",
    "The *oph_endif* operator simply closes a selection block \"if\".\n",
    "\n",
    "If we want to gather the PID of the output datacube produced in each of the two branches, we have to specify a dependency from both final tasks (*\\\"Subset data\\\"* and *\\\"Import and subset\\\"*) of each sub-workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_if += \"\"\"\n",
    "{\n",
    "                        \"name\": \"Selection block end\",\n",
    "                        \"operator\": \"oph_endif\",\n",
    "                        \"arguments\": [ ],\n",
    "                        \"dependencies\":\n",
    "                        [\n",
    "                                { \"task\": \"Subset data\"},\n",
    "                                { \"task\": \"Import and subset\" }\n",
    "                        ]\n",
    "                }\n",
    "        ]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the workflow arguments:\n",
    "- *if condition* ```$1```\n",
    "- *NetCDF filename* ```$2```\n",
    "- *variable* to be imported ```$3```\n",
    "- *subset filter* (lat|lon|time) ```$4```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition=0\n",
    "file=\"tasmax_day_CMCC-CESM_rcp85_r1i1p1_20960101-21001231.nc\"\n",
    "variable=\"tasmax\"\n",
    "subset=\"-50:10|20:140|150:240\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.client.wsubmit(workflow_if, condition ,file, variable, subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check produced datacube. As we can note:\n",
    "- if **conditon** equal **1** ---> datacube is imported and subsetted at the same time\n",
    "- else ---> datacube is first imported, then subsetted\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.list(level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the subsetted datacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsetted_cube = cube.Cube(pid='http://127.0.0.1/ophidia/.../...')\n",
    "subsetted_cube.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.deletecontainer(container=file,force='yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling task errors\n",
    "\n",
    "The Ophidia Workflow manager provides also support for handling errors in task executions.\n",
    "\n",
    "Four behaviours are supported by setting the proper value in the ```on_error``` argument:\n",
    "- *skip*: the task is skipped and execution continues on the descendant tasks\n",
    "- *continue*: the task and all depending task will be ignored, while other task will be executed\n",
    "- *break*: the workflow is interrupted (default)\n",
    "- *repeat N*: the task is re-executed N times\n",
    "\n",
    "In all the previous examples the default behaviour of interrupting the workflow in case of errors (*break*) was used.\n",
    "\n",
    "Note that the behaviour can be defined both at general workflow level and at the task level.\n",
    "\n",
    "We can now define a workflow where some of the tasks are expected to fail.  In particular, we want to import NetCDF files related to different years and compute the average on the time dimensions. Let's suppose we try to access a file that does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_handling_workflow = \"\"\"{\n",
    "        \"name\": \"error_handling\",\n",
    "        \"author\": \"CMCC\",\n",
    "        \"abstract\": \"Perform some basics operations using workflows\",\n",
    "        \"exec_mode\": \"sync\",\n",
    "        \"ncores\": \"1\",\n",
    "        \"cwd\": \"/\",\n",
    "        \"on_error\": \"$1\",\n",
    "        \"tasks\":\n",
    "        [\n",
    "                {\n",
    "                        \"name\": \"Create container\",\n",
    "                        \"operator\": \"oph_createcontainer\",\n",
    "                        \"arguments\": \n",
    "                        [\n",
    "                                \"container=workflow\",\n",
    "                                \"dim=lat|lon|time\",\n",
    "                                \"dim_type=double|double|double\",\n",
    "                                \"hierarchy=oph_base|oph_base|oph_time\"\n",
    "                        ]\n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"Start loop\",\n",
    "                        \"operator\": \"oph_for\",\n",
    "                        \"arguments\": \n",
    "                        [\n",
    "                                \"key=year\",\n",
    "                                \"values=2095|2096|2097\",\n",
    "                                \"parallel=yes\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Create container\"}\n",
    "                        ]\n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"Import\",\n",
    "                        \"operator\": \"oph_importnc\",\n",
    "                        \"on_error\": \"$2\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"src_path=/home/ophidia/notebooks/tasmax_day_CMCC-CESM_rcp85_r1i1p1_@{year}0101-@{year}1231.nc\",                           \n",
    "                                \"measure=tasmax\",\n",
    "                                \"container=workflow\",\n",
    "                                \"import_metadata=yes\",\n",
    "                                \"imp_dim=time\",\n",
    "                                \"imp_concept_level=d\",\n",
    "                                \"vocabulary=CF\",\n",
    "                                \"hierarchy=oph_base|oph_base|oph_time\",\n",
    "                                \"description=Max Temp @{year}\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Start loop\"}\n",
    "                        ]\n",
    "                },   \n",
    "                {\n",
    "                    \"name\": \"Reduce\",\n",
    "                    \"operator\": \"oph_reduce\",\n",
    "                    \"arguments\": [\n",
    "                        \"operation=avg\",\n",
    "                        \"description=Reduced cube\",\n",
    "                        \"dim=time\"\n",
    "                    ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Import\", \"type\": \"single\" }\n",
    "                        ]\n",
    "                        \n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"End loop year\",\n",
    "                        \"operator\": \"oph_endfor\",\n",
    "                        \"arguments\": [],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Reduce\", \"type\": \"all\" }\n",
    "                        ]\n",
    "                }\n",
    "        ]\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the global workflow attributes and the ```on_error```argument through the ```$1``` variable to control the general workflow execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_handling_workflow = \"\"\"{\n",
    "        \"name\": \"error_handling\",\n",
    "        \"author\": \"CMCC\",\n",
    "        \"abstract\": \"Perform some basics operations using workflows\",\n",
    "        \"exec_mode\": \"sync\",\n",
    "        \"ncores\": \"1\",\n",
    "        \"cwd\": \"/\",\n",
    "        \"on_error\": \"$1\",\n",
    "        \"tasks\":\n",
    "        [\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define then the first two tasks: *oph_createcontainer* and *oph_for*. \n",
    "\n",
    "Note that the ```values``` argument points to year (*2095*) for which we do not have any file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_handling_workflow += \"\"\"\n",
    "        {\n",
    "                \"name\": \"Create container\",\n",
    "                \"operator\": \"oph_createcontainer\",\n",
    "                \"arguments\": \n",
    "                [\n",
    "                        \"container=workflow\",\n",
    "                        \"dim=lat|lon|time\",\n",
    "                        \"dim_type=double|double|double\",\n",
    "                        \"hierarchy=oph_base|oph_base|oph_time\"\n",
    "                ]\n",
    "        },\n",
    "        {\n",
    "                \"name\": \"Start loop\",\n",
    "                \"operator\": \"oph_for\",\n",
    "                \"arguments\": \n",
    "                [\n",
    "                        \"key=year\",\n",
    "                        \"values=2095|2096|2097\",\n",
    "                        \"parallel=yes\"\n",
    "                ],\n",
    "                \"dependencies\": [\n",
    "                        { \"task\": \"Create container\"}\n",
    "                ]\n",
    "        },\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now specify the import operator. Here we are defining a specific ```on_error``` behaviour at the task level, with ```$2```, which supersedes the one define at the global level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_handling_workflow += \"\"\"\n",
    "        {\n",
    "                \"name\": \"Import\",\n",
    "                \"operator\": \"oph_importnc\",\n",
    "                \"on_error\": \"$2\",\n",
    "                \"arguments\":\n",
    "                [\n",
    "                        \"src_path=/home/ophidia/notebooks/tasmax_day_CMCC-CESM_rcp85_r1i1p1_@{year}0101-@{year}1231.nc\",                           \n",
    "                        \"measure=tasmax\",\n",
    "                        \"container=workflow\",\n",
    "                        \"import_metadata=yes\",\n",
    "                        \"imp_dim=time\",\n",
    "                        \"imp_concept_level=d\",\n",
    "                        \"vocabulary=CF\",\n",
    "                        \"hierarchy=oph_base|oph_base|oph_time\",\n",
    "                        \"description=Max Temp @{year}\"\n",
    "                ],\n",
    "                \"dependencies\": [\n",
    "                        { \"task\": \"Start loop\"}\n",
    "                ]\n",
    "        },   \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then specify the remaining tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_handling_workflow += \"\"\"\n",
    "                {\n",
    "                    \"name\": \"Reduce\",\n",
    "                    \"operator\": \"oph_reduce\",\n",
    "                    \"arguments\": [\n",
    "                        \"operation=avg\",\n",
    "                        \"description=Reduced cube\",\n",
    "                        \"dim=time\"\n",
    "                    ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Import\", \"type\": \"single\" }\n",
    "                        ]\n",
    "                        \n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"End loop year\",\n",
    "                        \"operator\": \"oph_endfor\",\n",
    "                        \"arguments\": [],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Reduce\", \"type\": \"all\" }\n",
    "                        ]\n",
    "                }\n",
    "        ]\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to run it with the default behaviour and see what happens..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.client.wsubmit(error_handling_workflow, \"break\", \"break\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could then set a specific behaviour for the whole workflow to skip a task failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.client.wsubmit(error_handling_workflow,\"skip\",\"skip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or define a specific one for the import operator we expect to fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.client.wsubmit(error_handling_workflow,\"skip\",\"continue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the cubes created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.list(level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.deletecontainer(container=\"workflow\",force='yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The virtual file system should now be \\\"clean\\\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
