{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical examples of simple workflow building and integration with PyOphidia module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, import **PyOphidia** module and setup a connection to the **Ophidia Server**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from PyOphidia import cube,client\n",
    "cube.Cube.setclient(read_env=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic workflow: import + subset + export\n",
    "\n",
    "The following JSON object is an example of workflow with 3 tasks: \n",
    "- one independent task: *Import* to import a NetCDF file into a datacube\n",
    "- two dependent tasks: *Subset* and *Export* to perform a subsetting operation along the dimensions of the  datacube and export the result into a new NetCDF file.\n",
    "\n",
    "\n",
    "<img src=\"1_Basic_workflow.svg\" alt=\"basic_workflow\" width=\"100\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"/data/output\"):\n",
    "    os.makedirs(\"/data/output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = \"\"\"{\n",
    "        \"name\": \"Basic workflow\",\n",
    "        \"author\": \"CMCC\",\n",
    "        \"abstract\": \"Perform some basics operations using workflows\",\n",
    "        \"exec_mode\": \"sync\",\n",
    "        \"ncores\": \"2\",\n",
    "        \"on_exit\": \"oph_delete\",\n",
    "        \"cwd\": \"/\",\n",
    "        \"tasks\":\n",
    "        [\n",
    "                {\n",
    "                        \"name\": \"Import\",\n",
    "                        \"operator\": \"oph_importnc\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"src_path=$1\",\n",
    "                                \"measure=$2\",\n",
    "                                \"import_metadata=yes\",\n",
    "                                \"imp_dim=time\",\n",
    "                                \"imp_concept_level=d\",\n",
    "                                \"vocabulary=CF\",\n",
    "                                \"hierarchy=oph_base|oph_base|oph_time\",\n",
    "                                \"description=Max Temp\"\n",
    "                        ]\n",
    "                },        \n",
    "                {\n",
    "                        \"name\": \"Subset\",\n",
    "                        \"operator\": \"oph_subset\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"subset_filter=JJA\",\n",
    "                                \"subset_dims=time\",\n",
    "                                \"subset_type=coord\",\n",
    "                                \"description=JJA\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Import\", \"type\": \"single\", \"argument\":\"cube\" }\n",
    "                        ]\n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"Export\",\n",
    "                        \"operator\": \"oph_exportnc2\",\n",
    "                        \"arguments\": [\n",
    "                            \"output_name=JJA\",\n",
    "                            \"output_path=/data/output\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                            { \"task\": \"Subset\", \"type\": \"single\"}\n",
    "                        ]\n",
    "                }\n",
    "        ]\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start building the first workflow step by step!\n",
    "\n",
    "First of all, we define several **global attributes**, which include a number of metadata and default parameters values common to all the tasks. \n",
    "Some of these keywords are mandatory:\n",
    "- **name**: the title of the workflow\n",
    "- **author**: the author’s name\n",
    "- **abstract**: a short description of the workflow\n",
    "\n",
    "The parameter **exec_mode** specifies the execution mode, synchronous or asynchronous, and it refers to the entire workflow, not to single tasks. In case of synchronous mode the workflow will be executed in a blocking way, so the submitter will have to wait until it will be finished to display the results. If the execution mode is asynchronous, the workflow will be processed in a non-blocking way (like a batch mode), allowing the submitter to immediately take the control and eventually submit other requests. After sending an asynchronous request, the user can get workflow output (if available) by exploiting the **view** command.\n",
    "\n",
    "Since a lot of tasks could be launched in parallel, an important parameter is the number of cores per task (**ncores**) which specifies the default value to be associated with all the workflow’s tasks. This value can be overridden with another one tailored to a task with a different behaviour.\n",
    "\n",
    "By using the **on_exit** parameter the user can select the cubes that will be dropped out when a workflow ends.\n",
    "The default value of *on_exit* can be set as global attribute. Admitted values are:\n",
    "- oph_delete: remove the output cube\n",
    "- oph_deletecontainer: remove the output container (valid only for OPH_CREATECONTAINER)\n",
    "- nop: no operation has to be applied (default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = \"\"\"{\n",
    "        \"name\": \"Basic workflow\",\n",
    "        \"author\": \"CMCC\",\n",
    "        \"abstract\": \"Perform some basics operations using workflows\",\n",
    "        \"exec_mode\": \"sync\",\n",
    "        \"ncores\": \"2\",\n",
    "        \"on_exit\": \"oph_delete\",\n",
    "        \"cwd\": \"/\",\n",
    "        \"tasks\":\n",
    "        [\n",
    "\"\"\"        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each task is uniquely identified within the workflow by its **name** and it is related to a specific Ophidia Operator set as **operator**. According to that operator, the user can optionally insert a JSON array of key-value pairs (**arguments**) in order to call the operator with the appropriate arguments.\n",
    "\n",
    "The first task of the workflow is related to the *oph_importnc* operator. According to the operator specification, we need to specify the mandatory arguments: *src_path* and *measure*. In this example, the two arguments values, $1 and $2, will be replaced with the workflow input parameters before sending the request to the Ophidia Server.\n",
    "\n",
    "In addition, we can specify other arguments such as:\n",
    "- *import_metadata=yes* to import also metadata from the input NetCDF file\n",
    "- *imp_dim=time* to arrange data in order to operate on time series\n",
    "- *imp_concept_level=d* to set the concept level to *day*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow += \"\"\"\n",
    "\n",
    "{\n",
    "                        \"name\": \"Import\",\n",
    "                        \"operator\": \"oph_importnc\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"src_path=$1\",\n",
    "                                \"measure=$2\",\n",
    "                                \"import_metadata=yes\",\n",
    "                                \"imp_dim=time\",\n",
    "                                \"imp_concept_level=d\",\n",
    "                                \"vocabulary=CF\",\n",
    "                                \"hierarchy=oph_base|oph_base|oph_time\",\n",
    "                                \"description=Max Temp\"\n",
    "                        ]\n",
    "                }, \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second task is related to the *oph_subset* operator. \n",
    "\n",
    "For example, we can consider the whole spatial domain and specify a subset only on the time range, as indicated by the *subset_dims* parameter.\n",
    "We can select a particular season by using the corresponding code for the *subset filter* argument:\n",
    "    - DJF for winter\n",
    "    - MAM for spring\n",
    "    - JJA for summer\n",
    "    - SON for autumn\n",
    "\n",
    "The input cube is the output cube of the previous task (*Import*), so we have to specify a dependency between these two tasks.\n",
    "\n",
    "The dependencies can be specified by a JSON array put in the section **dependencies** of the dependent task (child). Each item of this array is a JSON object related to a specific parent task which the child depends on. \n",
    "In general, it reports: the name of the **parent task** and the **type** of the dependency:\n",
    "- *single*: if the child task exploits only one output of parent task;\n",
    "- *all*: if the child task processes all the outputs of parent task (e.g. dependencies between massive operations);\n",
    "- *embedded*: to specify a simple flow dependency (the child task has to begin only after the parent task has finished), with no dependency on the outputs of the parent task\n",
    "\n",
    "By default, for the first two options, the **argument** parameter of a dependency is set to **cube** so that it can be usually omitted.\n",
    "It is also possible to specify the particular operator argument whose value is depending on the output produced by another task.\n",
    "\n",
    "In the example below, we set\n",
    "- **task** equal to the name of the parent task (i.e. Import);\n",
    "- **type** equal to **single** since the child task will use only one output from the parent task.\n",
    "\n",
    "We can omit **argument** since by default the *cube* parameter of the *oph_subset* operator will be set to the PID of the cube imported in the *Import* task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow+= \"\"\"\n",
    "\n",
    "{\n",
    "                        \"name\": \"Subset\",\n",
    "                        \"operator\": \"oph_subset\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"subset_filter=JJA\",\n",
    "                                \"subset_dims=time\",\n",
    "                                \"subset_type=coord\",\n",
    "                                \"description=JJA\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Import\", \"type\": \"single\", \"argument\": \"cube\" }\n",
    "                        ]\n",
    "                },\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can export the subsetted cube by using the *oph_exportnc2* operator.\n",
    "\n",
    "In this case we have to:\n",
    "- provide arguments for the oph_exportnc2 operator: *output_name* and *output_path*\n",
    "- set a **'single'** dependency from the previous (i.e. Subset) task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow+= \"\"\"\n",
    "\n",
    "{\n",
    "                        \"name\": \"Export\",\n",
    "                        \"operator\": \"oph_exportnc2\",\n",
    "                        \"arguments\": [\n",
    "                            \"output_name=JJA\",\n",
    "                            \"output_path=/data/output\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                            { \"task\": \"Subset\", \"type\": \"single\"}\n",
    "                        ]\n",
    "                }\n",
    "        ]\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JSON object related to our first workflow is ready!\n",
    "\n",
    "We have to define the workflow input arguments:\n",
    "- path to nc file\n",
    "- nc filename \n",
    "- variable to be imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/data/\"\n",
    "file=\"tasmax_day_CMCC-CESM_rcp85_r1i1p1_20960101-21001231.nc\"\n",
    "variable=\"tasmax\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To submit the workflow within the notebook we can use the **wsubmit** PyOphidia method, providing:\n",
    "- the workflow name\n",
    "- the input parameters\n",
    "\n",
    "In our first example the workflow has two parameters:\n",
    "- the source path used in the oph_importnc operator to load the NetCDF file\n",
    "- the variable name use to set the measure argument in the import operator\n",
    "\n",
    "The output of the workflow is a report with the final status of each task. By default three objects are included:\n",
    " - a text object **Workflow Status**, which reports the workflow status;\n",
    " - a table object **Workflow Progress**, which reports the total number of tasks and the number of completed tasks;\n",
    " - a table object **Workflow Task List**, which reports a list with some information about each task: job identifier, Marker ID, Task Name, Task Type (simple or massive) and Task Status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.client.wsubmit(workflow,path+file, variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check exported file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -lh /data/output | grep \"\\.nc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workspace is already empty (we have no datacubes) because we've used **\"on_exit\": \"oph_delete\"** as global attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.list(level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just need to remove the container automatically created by the *oph_importnc* operator. By default, the container name is equal to the name of the imported file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.deletecontainer(container=file,force='yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Workflows: iterative and parallel interfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now consider a slightly more complex workflow, in which we are going to:\n",
    "- import and subset multiple NetCDF files in parallel\n",
    "- merge all the subsetted datacubes\n",
    "- perform a reduction (avg, max, min, ...) operation\n",
    "- export the output datacube\n",
    "\n",
    "<img src=\"2_Iterative_parallel.svg\" alt=\"iterative_parallel\" width=\"800\">\n",
    "\n",
    "As input files, we can use the daily NetCDF files produced by the CMCC-CM model and related to the *tasmax* variable for the years 2011-2013. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JSON object associated to the workflow consists of several tasks. In addition to the Ophidia Data Import/Export & Analysis operators, we are going to exploit the **OPH_FOR** and **OPH_ENDFOR** flow control operators to implement a \"for\" loop. \n",
    "\n",
    "Unlike other Ophidia operators, they does not operate on data or metadata, but they could be adopted to set particular flow control rules for the Workflow manager. In particular, they are used to begin/end a sub-section that has to be executed several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_loop = \"\"\"{\n",
    "        \"name\": \"Loop operations\",\n",
    "        \"author\": \"CMCC\",\n",
    "        \"abstract\": \"Perform some basics operations using workflows\",\n",
    "        \"exec_mode\": \"sync\",\n",
    "        \"ncores\": \"1\",\n",
    "        \"on_exit\": \"oph_delete\",\n",
    "        \"cwd\": \"/\",\n",
    "        \"tasks\":\n",
    "        [\n",
    "                {\n",
    "                        \"name\": \"Create container\",\n",
    "                        \"operator\": \"oph_createcontainer\",\n",
    "                        \"arguments\": \n",
    "                        [\n",
    "                                \"container=workflow\",\n",
    "                                \"dim=lat|lon|time\",\n",
    "                                \"dim_type=double|double|double\",\n",
    "                                \"hierarchy=oph_base|oph_base|oph_time\"\n",
    "                        ]\n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"Start loop\",\n",
    "                        \"operator\": \"oph_for\",\n",
    "                        \"arguments\": \n",
    "                        [\n",
    "                                \"key=year\",\n",
    "                                \"values=2011|2012|2013\",\n",
    "                                \"parallel=yes\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Create container\"}\n",
    "                        ]\n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"Import\",\n",
    "                        \"operator\": \"oph_importnc\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"src_path=/data/tasmax_day_CMCC-CM_rcp85_r1i1p1_@{year}0101-@{year}1231.nc\",                           \n",
    "                                \"measure=$1\",\n",
    "                                \"container=workflow\",\n",
    "                                \"import_metadata=yes\",\n",
    "                                \"imp_dim=time\",\n",
    "                                \"imp_concept_level=d\",\n",
    "                                \"vocabulary=CF\",\n",
    "                                \"hierarchy=oph_base|oph_base|oph_time\",\n",
    "                                \"description=Max Temp @{year}\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Start loop\"}\n",
    "                        ]\n",
    "                },   \n",
    "                {\n",
    "                        \"name\": \"Subset\",\n",
    "                        \"operator\": \"oph_subset\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"subset_filter=JJA\",\n",
    "                                \"subset_dims=time\",\n",
    "                                \"subset_type=coord\",\n",
    "                                \"description=JJA @{year}\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Import\", \"type\": \"single\" },\n",
    "                                { \"task\": \"Start loop\"}\n",
    "                        ]\n",
    "                        \n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"End loop year\",\n",
    "                        \"operator\": \"oph_endfor\",\n",
    "                        \"arguments\": [],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Subset\", \"type\": \"all\" }\n",
    "                        ]\n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"Merge\",\n",
    "                        \"operator\": \"oph_mergecubes\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"description=Merged cube\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"End loop year\", \"type\": \"all\", \"argument\": \"cubes\" }\n",
    "                        ]\n",
    "                        \n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"Reduce\",\n",
    "                    \"operator\": \"oph_reduce\",\n",
    "                    \"arguments\": [\n",
    "                        \"operation=avg\",\n",
    "                        \"description=Reduced cube\",\n",
    "                        \"dim=time\"\n",
    "                    ],\n",
    "                    \"dependencies\": [\n",
    "                        { \"task\": \"Merge\", \"type\": \"single\" }\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"Export\",\n",
    "                        \"operator\": \"oph_exportnc2\",\n",
    "                        \"arguments\": [\n",
    "                            \"output_name=avg_JJA\",\n",
    "                            \"output_path=/data/output\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                            { \"task\": \"Reduce\", \"type\": \"single\"}\n",
    "                        ]\n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"Delete container\",\n",
    "                        \"operator\": \"oph_deletecontainer\",\n",
    "                        \"arguments\": [\n",
    "                                \"container=workflow\",\n",
    "                                \"force=yes\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Export\", \"type\": \"embedded\" }\n",
    "                        ]\n",
    "                }\n",
    "                \n",
    "        ]\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the workflow step by step.\n",
    "\n",
    "In the following cell we define some global attributes as in the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_loop = \"\"\"{\n",
    "        \"name\": \"Loop operations\",\n",
    "        \"author\": \"CMCC\",\n",
    "        \"abstract\": \"Perform some basics operations using workflows\",\n",
    "        \"exec_mode\": \"sync\",\n",
    "        \"ncores\": \"1\",\n",
    "        \"on_exit\": \"oph_delete\",\n",
    "        \"cwd\": \"/\",\n",
    "        \"tasks\":\n",
    "        [\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a new container**\n",
    "\n",
    "This is the first task, so it has no dependencies. We just have to provide the proper arguments to the *oph_createcontainer* operator:\n",
    "- the container name\n",
    "- the name and the type of the dimensions allowed\n",
    "- the concept hierarchy name of the dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_loop += \"\"\"\n",
    "{\n",
    "                        \"name\": \"Create container\",\n",
    "                        \"operator\": \"oph_createcontainer\",\n",
    "                        \"arguments\": \n",
    "                        [\n",
    "                                \"container=workflow\",\n",
    "                                \"dim=lat|lon|time\",\n",
    "                                \"dim_type=double|double|double\",\n",
    "                                \"hierarchy=oph_base|oph_base|oph_time\"\n",
    "                        ]\n",
    "                },\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FOR statement**\n",
    "\n",
    "The OPH_FOR operator is used to configure the iterative block and, in particular, to set the number N of loops to be executed. By this aim, we have to provide an ordered list of N labels to be assigned to cycles in order to distinguish one cycle from another one. The list is assigned to the **values** parameter, separating each value by |. \n",
    "\n",
    "In our example, we provide a list of years in order to import the corresponding NetCDF file in the next task.\n",
    "\n",
    "A name has to be associated to the list values by setting the **key** parameter (e.g. *year*), which is used in the inner tasks in the form **@{key_name}** to access the current value of the counter/label. \n",
    "\n",
    "We set **parallel** to *\\\"yes\\\"* for parallel processing.\n",
    "If the option is enabled for a OPH_FOR, the engine, before executing the workflow, transforms it into an equivalent version in which iterative blocks are expanded into N independent sub-workflows, where N is the number of initial iterations. The new workflow is then executed taking into account the usual rules based on task dependencies.\n",
    "\n",
    "Finally, we define a simple flow dependency (*type=embedded*), since this task has to begin only after the previous \"Create container\" task has finished.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_loop += \"\"\"\n",
    "{\n",
    "                        \"name\": \"Start loop\",\n",
    "                        \"operator\": \"oph_for\",\n",
    "                        \"arguments\": \n",
    "                        [\n",
    "                                \"key=year\",\n",
    "                                \"values=2011|2012|2013\",\n",
    "                                \"parallel=yes\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Create container\", \"type\":\"embedded\"}\n",
    "                        ]\n",
    "                },\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import and subset multiple datacubes in parallel**\n",
    "\n",
    "The two inner tasks to be repeated (import and subset) have to depend on OPH_FOR directly or indirectly, namely they depend on other tasks in the iterative block. \n",
    "\n",
    "Setting the parameters of these tasks the user is able to exploit the value of the label associated with current iteration. \n",
    "\n",
    "*IMPORT task*\n",
    "\n",
    "The **src_path** as well the **description** parameters in the *oph_importnc* operator are defined in a parametrized way to get the current value of the **year** key for each iteration.\n",
    "This task has a simple flow dependency from the \"Start loop\" task in order to start after the \"Start loop\" task and retrieve the right value of the label associated with current iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_loop += \"\"\"\n",
    "{\n",
    "                        \"name\": \"Import\",\n",
    "                        \"operator\": \"oph_importnc\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"src_path=/data/tasmax_day_CMCC-CM_rcp85_r1i1p1_@{year}0101-@{year}1231.nc\",                           \n",
    "                                \"measure=$1\",\n",
    "                                \"container=workflow\",\n",
    "                                \"import_metadata=yes\",\n",
    "                                \"imp_dim=time\",\n",
    "                                \"imp_concept_level=d\",\n",
    "                                \"vocabulary=CF\",\n",
    "                                \"hierarchy=oph_base|oph_base|oph_time\",\n",
    "                                \"description=Max Temp @{year}\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Start loop\", \"type\":\"embedded\"}\n",
    "                        ]\n",
    "                }, \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*SUBSET task*\n",
    "\n",
    "This task has two dependencies:\n",
    "- a *flow* dependency from the \"Start loop\" task to get the current value of the label, which is used in the *description* parameter\n",
    "- a *single* dependency from the \"Import\" task since each subset operation has to be performed on the corresponding datacube imported at the previous import step, so the output from the *Import* task is the input for the *Subset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_loop += \"\"\"\n",
    "{\n",
    "                        \"name\": \"Subset\",\n",
    "                        \"operator\": \"oph_subset\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"subset_filter=JJA\",\n",
    "                                \"subset_dims=time\",\n",
    "                                \"subset_type=coord\",\n",
    "                                \"description=JJA @{year}\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Import\", \"type\": \"single\" },\n",
    "                                { \"task\": \"Start loop\", \"type\":\"embedded\"}\n",
    "                        ]\n",
    "                        \n",
    "                },\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*End loop*\n",
    "\n",
    "The OPH_ENDFOR operator ends an iterative block, has no arguments and depends on the inner tasks.\n",
    "\n",
    "In our example, it depends on the \"Subset\" task and the dependency type is **all**. In this way, it can gather PIDs of all cubes generated by the (subset) inner task and transfer them to next tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_loop += \"\"\"\n",
    "{\n",
    "                        \"name\": \"End loop\",\n",
    "                        \"operator\": \"oph_endfor\",\n",
    "                        \"arguments\": [],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Subset\", \"type\": \"all\" }\n",
    "                        ]\n",
    "                },\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge all the subsetted datacubes into a single datacube**\n",
    "\n",
    "All the subsetted datacubes can be now merged into a single datacube by using the **oph_mergecubes** operator: the resulting datacube will contain the JJA subset for each of the imported years. \n",
    "\n",
    "As for the previous task, we need to specify an **all** dependency to get all the datacubes PIDs from the previous task. In addition, we have to set the *argument* parameter to *cubes* so that the value of the *cubes* parameter for the *oph_mergecubes* operator will be set to a list of pipe-separated PIDs retrieved from the \"End loop\" task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_loop += \"\"\"\n",
    "{\n",
    "                        \"name\": \"Merge\",\n",
    "                        \"operator\": \"oph_mergecubes\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"description=Merged cube\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"End loop\", \"type\": \"all\", \"argument\": \"cubes\" }\n",
    "                        ]\n",
    "                        \n",
    "                },\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform a reduction operation**\n",
    "\n",
    "Starting from the merged datacube, we can perform a reduction operation with respect to the implicit dimension (time).\n",
    "\n",
    "We just need to define a *single* dependency between the **Reduce** task and the previous **Merge** task.\n",
    "\n",
    "The reduced cube will contain the average value for the tasmax variable over the 2011-2013 JJA period for each point in the spatial domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_loop += \"\"\"\n",
    "{\n",
    "                    \"name\": \"Reduce\",\n",
    "                    \"operator\": \"oph_reduce\",\n",
    "                    \"arguments\": [\n",
    "                        \"operation=avg\",\n",
    "                        \"description=Reduced cube\",\n",
    "                        \"dim=time\"\n",
    "                    ],\n",
    "                    \"dependencies\": [\n",
    "                        { \"task\": \"Merge\", \"type\": \"single\" }\n",
    "                    ]\n",
    "                },\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export the averaged datacube**\n",
    "\n",
    "In a similar way, we can define an *\\\"Export\\\"* task that depends on the *\"\\Reduce\\\"* task to export data into a single NetCDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_loop += \"\"\"\n",
    "{\n",
    "                        \"name\": \"Export\",\n",
    "                        \"operator\": \"oph_exportnc2\",\n",
    "                        \"arguments\": [\n",
    "                            \"output_name=avg_JJA\",\n",
    "                            \"output_path=/data/output\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                            { \"task\": \"Reduce\", \"type\": \"single\"}\n",
    "                        ]\n",
    "                },\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Empty workspace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_loop += \"\"\"\n",
    "{\n",
    "                        \"name\": \"Delete container\",\n",
    "                        \"operator\": \"oph_deletecontainer\",\n",
    "                        \"arguments\": [\n",
    "                                \"container=workflow\",\n",
    "                                \"force=yes\"\n",
    "                        ],\n",
    "                        \"dependencies\": [\n",
    "                                { \"task\": \"Export\", \"type\": \"embedded\" }\n",
    "                        ]\n",
    "                }\n",
    "                \n",
    "        ]\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run workflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.client.wsubmit(workflow_loop,variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -lh /data/output | grep \"\\.nc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. Workflows: Selection interface\n",
    "\n",
    "The Selection interface provides further flexibility by enabling the Workflow manager to execute one or more tasks based on boolean conditions that could be checked at run-time and depend on input parameters, data, metadata, etc.\n",
    "\n",
    "The development of the Selection interface involved the design of new Ophidia operators:\n",
    " - OPH_IF\n",
    " - OPH_ELSEIF\n",
    " - OPH_ELSE\n",
    " - OPH_ENDIF\n",
    "\n",
    "Similarly to other flow control operators, they does not process data or metadata directly, but they could be adopted to enable (or to skip) the execution of a set of tasks based on run-time conditions.\n",
    "\n",
    "In the following workflow, we'll consider a selection statement with two selection blocks.\n",
    "\n",
    "<img src=\"3_Selection_Interface.svg\" alt=\"selection_interface\" width=\"800\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_if = \"\"\"{\n",
    "        \"name\": \"Selection Interface\",\n",
    "        \"author\": \"CMCC\",\n",
    "        \"abstract\": \"Selection statement with two selection blocks\",\n",
    "        \"exec_mode\": \"sync\",\n",
    "        \"ncores\": \"1\",\n",
    "        \"tasks\":\n",
    "        [\n",
    "                {\n",
    "                        \"name\": \"IF block\",\n",
    "                        \"operator\": \"oph_if\",\n",
    "                        \"arguments\": [ \"condition=$1\" ]\n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"Import and subset\",\n",
    "                        \"operator\": \"oph_importnc\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"src_path=$2\",\n",
    "                                \"measure=$3\",\n",
    "                                \"import_metadata=yes\",\n",
    "                                \"imp_dim=time\",\n",
    "                                \"imp_concept_level=d\",\n",
    "                                \"vocabulary=CF\",\n",
    "                                \"hierarchy=oph_base|oph_base|oph_time\",\n",
    "                                \"description=Max Temp imported and subsetted\",\n",
    "                                \"subset_dims=lat|lon|time\",\n",
    "                                \"subset_filter=$4\",\n",
    "                                \"subset_type=coord\"\n",
    "                        ],\n",
    "                        \"dependencies\":\n",
    "                        [\n",
    "                                { \"task\": \"IF block\" }\n",
    "                        ]\n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"ELSE block\",\n",
    "                        \"operator\": \"oph_else\",\n",
    "                        \"arguments\": [  ],\n",
    "                        \"dependencies\":\n",
    "                        [\n",
    "                                { \"task\": \"IF block\" }\n",
    "                        ]\n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"Import data\",\n",
    "                        \"operator\": \"oph_importnc\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"src_path=$2\",\n",
    "                                \"measure=$3\",\n",
    "                                \"import_metadata=yes\",\n",
    "                                \"imp_dim=time\",\n",
    "                                \"imp_concept_level=d\",\n",
    "                                \"vocabulary=CF\",\n",
    "                                \"hierarchy=oph_base|oph_base|oph_time\",\n",
    "                                \"description=Max Temp imported\"\n",
    "                        ],\n",
    "                        \"dependencies\":\n",
    "                        [\n",
    "                                { \"task\": \"ELSE block\" }\n",
    "                        ]\n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"Subset data\",\n",
    "                        \"operator\": \"oph_subset\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                               \"subset_dims=lat|lon|time\",\n",
    "                               \"subset_filter=$4\",\n",
    "                               \"subset_type=coord\",\n",
    "                               \"description=Max Temp subsetted\" \n",
    "                        ],\n",
    "                        \"dependencies\":\n",
    "                        [\n",
    "                                { \"task\": \"Import data\",\n",
    "                                  \"type\": \"single\" }\n",
    "                        ]\n",
    "                },\n",
    "                {\n",
    "                        \"name\": \"Selection block end\",\n",
    "                        \"operator\": \"oph_endif\",\n",
    "                        \"arguments\": [ ],\n",
    "                        \"dependencies\":\n",
    "                        [\n",
    "                                { \"task\": \"Subset data\"},\n",
    "                                { \"task\": \"Import and subset\" }\n",
    "                        ]\n",
    "                }\n",
    "        ]\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Workflow global attributes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_if = \"\"\"{\n",
    "        \"name\": \"Selection Interface\",\n",
    "        \"author\": \"CMCC\",\n",
    "        \"abstract\": \"Selection statement with two selection blocks\",\n",
    "        \"exec_mode\": \"sync\",\n",
    "        \"ncores\": \"1\",\n",
    "        \"tasks\":\n",
    "        [\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IF block**\n",
    "\n",
    "The selection interface is used to code two possible implementations of a task that imports data into the Ophidia platform from an external source:\n",
    " \n",
    "     A) import only the subset from the input file\n",
    "     B) import all the dataset and then extract a data subset\n",
    "\n",
    "The actual implementation to be adopted is selected by means of the input parameter $1: a numerical non-zero value for option A, 0 for option B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_if += \"\"\"\n",
    "{\n",
    "                        \"name\": \"IF block\",\n",
    "                        \"operator\": \"oph_if\",\n",
    "                        \"arguments\": [ \"condition=$1\" ]\n",
    "                },\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CASE A: Import only the subset from the input file**\n",
    "\n",
    "In general, the set of tasks belonging to the branch that begins from OPH_IF and ends to OPH_ENDIF is the sub-workflow to be executed in case the condition set for OPH_IF is satisfied.\n",
    "\n",
    "In our example, there is only one task, named *\\\"Import and subset\\\"*, which is related to the *oph_importnc* operator and has a flow dependency from the \"IF block\" task.\n",
    "\n",
    "The *src_path* and the *measure* arguments will be set according to the second and third workflow input arguments.\n",
    "\n",
    "To import only a subset from the input file we have to specify in addition the following parameters:\n",
    "- **subset_dims**: the dimension names used for the subsetting\n",
    "- **subset_type=coord** so that the filter is considered on dimension values\n",
    "- **subset_filter**: list of pipe-separated filters associated to each dimension specified in *subset_dims* (set according to the fourth workflow input argument)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_if += \"\"\"\n",
    "{\n",
    "                        \"name\": \"Import and subset\",\n",
    "                        \"operator\": \"oph_importnc\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"src_path=$2\",\n",
    "                                \"measure=$3\",\n",
    "                                \"import_metadata=yes\",\n",
    "                                \"imp_dim=time\",\n",
    "                                \"imp_concept_level=d\",\n",
    "                                \"vocabulary=CF\",\n",
    "                                \"hierarchy=oph_base|oph_base|oph_time\",\n",
    "                                \"description=Max Temp imported and subsetted\",\n",
    "                                \"subset_dims=lat|lon|time\",\n",
    "                                \"subset_filter=$4\",\n",
    "                                \"subset_type=coord\"\n",
    "                        ],\n",
    "                        \"dependencies\":\n",
    "                        [\n",
    "                                { \"task\": \"IF block\" }\n",
    "                        ]\n",
    "                },\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ELSE block**\n",
    "\n",
    "The task with the OPH_ELSE operator has to be a child of the task with the OPH_IF operator. It has no arguments: it simply starts the last sub-block of a selection block \"if\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_if += \"\"\"\n",
    "{\n",
    "                        \"name\": \"ELSE block\",\n",
    "                        \"operator\": \"oph_else\",\n",
    "                        \"arguments\": [  ],\n",
    "                        \"dependencies\":\n",
    "                        [\n",
    "                                { \"task\": \"IF block\" }\n",
    "                        ]\n",
    "                },\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CASE B: import all the dataset and then extract a data subset**\n",
    "\n",
    "The set of tasks belonging to the branch that begins from OPH_ELSE and ends to OPH_ENDIF is the sub-workflow to be executed in case the condition set for OPH_IF is NOT satisfied.\n",
    "\n",
    "In our example, we have two tasks:\n",
    "- the first one, **\\\"Import data\\\"**, is related to the *oph_importnc* operator and is child of the task with the \"OPH_ELSE\" operator.\n",
    "- the second one,**\\\"Subset data\\\"**, is related to the *oph_subset* operator and has a \"single\" dependency from the \"Import data\" task since the input datacube to be subsetted is the datacube generated from the import task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_if += \"\"\"\n",
    "{\n",
    "                        \"name\": \"Import data\",\n",
    "                        \"operator\": \"oph_importnc\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                                \"src_path=$2\",\n",
    "                                \"measure=$3\",\n",
    "                                \"import_metadata=yes\",\n",
    "                                \"imp_dim=time\",\n",
    "                                \"imp_concept_level=d\",\n",
    "                                \"vocabulary=CF\",\n",
    "                                \"hierarchy=oph_base|oph_base|oph_time\",\n",
    "                                \"description=Max Temp imported\"\n",
    "                        ],\n",
    "                        \"dependencies\":\n",
    "                        [\n",
    "                                { \"task\": \"ELSE block\" }\n",
    "                        ]\n",
    "                },\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_if += \"\"\"\n",
    "{\n",
    "                        \"name\": \"Subset data\",\n",
    "                        \"operator\": \"oph_subset\",\n",
    "                        \"arguments\":\n",
    "                        [\n",
    "                               \"subset_dims=lat|lon|time\",\n",
    "                               \"subset_filter=$4\",\n",
    "                               \"subset_type=coord\",\n",
    "                               \"description=Max Temp subsetted\" \n",
    "                        ],\n",
    "                        \"dependencies\":\n",
    "                        [\n",
    "                                { \"task\": \"Import data\",\n",
    "                                  \"type\": \"single\" }\n",
    "                        ]\n",
    "                },\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ENDIF block**\n",
    "\n",
    "The *oph_endif* operator simply closes a selection block \"if\".\n",
    "\n",
    "If we want to gather the PID of the output datacube produced in each of the two branches, we have to specify a dependency from both final tasks (*\\\"Subset data\\\"* and *\\\"Import and subset\\\"*) of each sub-workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_if += \"\"\"\n",
    "{\n",
    "                        \"name\": \"Selection block end\",\n",
    "                        \"operator\": \"oph_endif\",\n",
    "                        \"arguments\": [ ],\n",
    "                        \"dependencies\":\n",
    "                        [\n",
    "                                { \"task\": \"Subset data\"},\n",
    "                                { \"task\": \"Import and subset\" }\n",
    "                        ]\n",
    "                }\n",
    "        ]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define input argument:\n",
    "- path to nc file\n",
    "- nc filename\n",
    "- variable to be imported \n",
    "- subset filter (lat|lon|time)\n",
    "- flag to be evaluated by the IF..ELSE statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/data/\"\n",
    "file=\"tasmax_day_CMCC-CESM_rcp85_r1i1p1_20960101-21001231.nc\"\n",
    "variable=\"tasmax\"\n",
    "lat_lon_time=\"-50:10|20:140|150:240\"\n",
    "import_and_subset=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.client.wsubmit(workflow_if,import_and_subset,path+file, variable,lat_lon_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check produced datacube. As we can note:\n",
    "- if **import_and_subset** equal **1** ---> datacube is imported and subsetted at the same time\n",
    "- else ---> datacube is first imported, then subsetted\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.list(level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check subsetted datacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsetted_cube = cube.Cube(pid='...')\n",
    "subsetted_cube.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.deletecontainer(container=file,force='yes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
